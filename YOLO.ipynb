{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "\n",
    "import numpy\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.nn.functional import cross_entropy, one_hot\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "\n",
    "\n",
    "def setup_seed():\n",
    "    \"\"\"\n",
    "    Setup random seed.\n",
    "    \"\"\"\n",
    "    random.seed(0)\n",
    "    numpy.random.seed(0)\n",
    "    torch.manual_seed(0)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "def setup_multi_processes():\n",
    "    \"\"\"\n",
    "    Setup multi-processing environment variables.\n",
    "    \"\"\"\n",
    "    import cv2\n",
    "    from os import environ\n",
    "    from platform import system\n",
    "\n",
    "    # set multiprocess start method as `fork` to speed up the training\n",
    "    if system() != 'Windows':\n",
    "        torch.multiprocessing.set_start_method('fork', force=True)\n",
    "\n",
    "    # disable opencv multithreading to avoid system being overloaded\n",
    "    cv2.setNumThreads(0)\n",
    "\n",
    "    # setup OMP threads\n",
    "    if 'OMP_NUM_THREADS' not in environ:\n",
    "        environ['OMP_NUM_THREADS'] = '1'\n",
    "\n",
    "    # setup MKL threads\n",
    "    if 'MKL_NUM_THREADS' not in environ:\n",
    "        environ['MKL_NUM_THREADS'] = '1'\n",
    "\n",
    "\n",
    "def scale(coords, shape1, shape2, ratio_pad=None):\n",
    "    if ratio_pad is None:  # calculate from img0_shape\n",
    "        gain = min(shape1[0] / shape2[0], shape1[1] / shape2[1])  # gain  = old / new\n",
    "        pad = (shape1[1] - shape2[1] * gain) / 2, (shape1[0] - shape2[0] * gain) / 2  # wh padding\n",
    "    else:\n",
    "        gain = ratio_pad[0][0]\n",
    "        pad = ratio_pad[1]\n",
    "\n",
    "    coords[:, [0, 2]] -= pad[0]  # x padding\n",
    "    coords[:, [1, 3]] -= pad[1]  # y padding\n",
    "    coords[:, :4] /= gain\n",
    "\n",
    "    coords[:, 0].clamp_(0, shape2[1])  # x1\n",
    "    coords[:, 1].clamp_(0, shape2[0])  # y1\n",
    "    coords[:, 2].clamp_(0, shape2[1])  # x2\n",
    "    coords[:, 3].clamp_(0, shape2[0])  # y2\n",
    "    return coords\n",
    "\n",
    "\n",
    "def make_anchors(x, strides, offset=0.5):\n",
    "    \"\"\"\n",
    "    Generate anchors from features\n",
    "    \"\"\"\n",
    "    assert x is not None\n",
    "    anchor_points, stride_tensor = [], []\n",
    "    for i, stride in enumerate(strides):\n",
    "        _, _, h, w = x[i].shape\n",
    "        sx = torch.arange(end=w, dtype=x[i].dtype, device=x[i].device) + offset  # shift x\n",
    "        sy = torch.arange(end=h, dtype=x[i].dtype, device=x[i].device) + offset  # shift y\n",
    "        sy, sx = torch.meshgrid(sy, sx)\n",
    "        anchor_points.append(torch.stack((sx, sy), -1).view(-1, 2))\n",
    "        stride_tensor.append(torch.full((h * w, 1), stride, dtype=x[i].dtype, device=x[i].device))\n",
    "    return torch.cat(anchor_points), torch.cat(stride_tensor)\n",
    "\n",
    "\n",
    "def box_iou(box1, box2):\n",
    "    # https://github.com/pytorch/vision/blob/master/torchvision/ops/boxes.py\n",
    "    \"\"\"\n",
    "    Return intersection-over-union (Jaccard index) of boxes.\n",
    "    Both sets of boxes are expected to be in (x1, y1, x2, y2) format.\n",
    "    Arguments:\n",
    "        box1 (Tensor[N, 4])\n",
    "        box2 (Tensor[M, 4])\n",
    "    Returns:\n",
    "        iou (Tensor[N, M]): the NxM matrix containing the pairwise\n",
    "            IoU values for every element in boxes1 and boxes2\n",
    "    \"\"\"\n",
    "\n",
    "    # intersection(N,M) = (rb(N,M,2) - lt(N,M,2)).clamp(0).prod(2)\n",
    "    (a1, a2), (b1, b2) = box1[:, None].chunk(2, 2), box2.chunk(2, 1)\n",
    "    intersection = (torch.min(a2, b2) - torch.max(a1, b1)).clamp(0).prod(2)\n",
    "\n",
    "    # IoU = intersection / (area1 + area2 - intersection)\n",
    "    box1 = box1.T\n",
    "    box2 = box2.T\n",
    "\n",
    "    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "\n",
    "    return intersection / (area1[:, None] + area2 - intersection)\n",
    "\n",
    "\n",
    "def wh2xy_(x):\n",
    "    y = x.clone()\n",
    "    y[..., 0] = x[..., 0] - x[..., 2] / 2  # top left x\n",
    "    y[..., 1] = x[..., 1] - x[..., 3] / 2  # top left y\n",
    "    y[..., 2] = x[..., 0] + x[..., 2] / 2  # bottom right x\n",
    "    y[..., 3] = x[..., 1] + x[..., 3] / 2  # bottom right y\n",
    "    return y\n",
    "\n",
    "\n",
    "def non_max_suppression(prediction, conf_threshold=0.25, iou_threshold=0.45):\n",
    "    nc = prediction.shape[1] - 4  # number of classes\n",
    "    xc = prediction[:, 4:4 + nc].amax(1) > conf_threshold  # candidates\n",
    "\n",
    "    # Settings\n",
    "    max_wh = 7680  # (pixels) maximum box width and height\n",
    "    max_det = 300  # the maximum number of boxes to keep after NMS\n",
    "    max_nms = 30000  # maximum number of boxes into torchvision.ops.nms()\n",
    "\n",
    "    start = time.time()\n",
    "    outputs = [torch.zeros((0, 6), device=prediction.device)] * prediction.shape[0]\n",
    "    for index, x in enumerate(prediction):  # image index, image inference\n",
    "        # Apply constraints\n",
    "        x = x.transpose(0, -1)[xc[index]]  # confidence\n",
    "\n",
    "        # If none remain process next image\n",
    "        if not x.shape[0]:\n",
    "            continue\n",
    "\n",
    "        # Detections matrix nx6 (box, conf, cls)\n",
    "        box, cls = x.split((4, nc), 1)\n",
    "        # center_x, center_y, width, height) to (x1, y1, x2, y2)\n",
    "        box = wh2xy_(box)\n",
    "        if nc > 1:\n",
    "            i, j = (cls > conf_threshold).nonzero(as_tuple=False).T\n",
    "            x = torch.cat((box[i], x[i, 4 + j, None], j[:, None].float()), 1)\n",
    "        else:  # best class only\n",
    "            conf, j = cls.max(1, keepdim=True)\n",
    "            x = torch.cat((box, conf, j.float()), 1)[conf.view(-1) > conf_threshold]\n",
    "        # Check shape\n",
    "        if not x.shape[0]:  # no boxes\n",
    "            continue\n",
    "        # sort by confidence and remove excess boxes\n",
    "        x = x[x[:, 4].argsort(descending=True)[:max_nms]]\n",
    "\n",
    "        # Batched NMS\n",
    "        c = x[:, 5:6] * max_wh  # classes\n",
    "        boxes, scores = x[:, :4] + c, x[:, 4]  # boxes (offset by class), scores\n",
    "        i = torchvision.ops.nms(boxes, scores, iou_threshold)  # NMS\n",
    "        i = i[:max_det]  # limit detections\n",
    "        outputs[index] = x[i]\n",
    "        if (time.time() - start) > 0.5 + 0.05 * prediction.shape[0]:\n",
    "            print(f'WARNING ⚠️ NMS time limit {0.5 + 0.05 * prediction.shape[0]:.3f}s exceeded')\n",
    "            break  # time limit exceeded\n",
    "\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def smooth(y, f=0.05):\n",
    "    # Box filter of fraction f\n",
    "    nf = round(len(y) * f * 2) // 2 + 1  # number of filter elements (must be odd)\n",
    "    p = numpy.ones(nf // 2)  # ones padding\n",
    "    yp = numpy.concatenate((p * y[0], y, p * y[-1]), 0)  # y padded\n",
    "    return numpy.convolve(yp, numpy.ones(nf) / nf, mode='valid')  # y-smoothed\n",
    "\n",
    "\n",
    "def compute_ap(tp, conf, pred_cls, target_cls, eps=1e-16):\n",
    "    \"\"\"\n",
    "    Compute the average precision, given the recall and precision curves.\n",
    "    Source: https://github.com/rafaelpadilla/Object-Detection-Metrics.\n",
    "    # Arguments\n",
    "        tp:  True positives (nparray, nx1 or nx10).\n",
    "        conf:  Object-ness value from 0-1 (nparray).\n",
    "        pred_cls:  Predicted object classes (nparray).\n",
    "        target_cls:  True object classes (nparray).\n",
    "    # Returns\n",
    "        The average precision\n",
    "    \"\"\"\n",
    "    # Sort by object-ness\n",
    "    i = numpy.argsort(-conf)\n",
    "    tp, conf, pred_cls = tp[i], conf[i], pred_cls[i]\n",
    "\n",
    "    # Find unique classes\n",
    "    unique_classes, nt = numpy.unique(target_cls, return_counts=True)\n",
    "    nc = unique_classes.shape[0]  # number of classes, number of detections\n",
    "\n",
    "    # Create Precision-Recall curve and compute AP for each class\n",
    "    p = numpy.zeros((nc, 1000))\n",
    "    r = numpy.zeros((nc, 1000))\n",
    "    ap = numpy.zeros((nc, tp.shape[1]))\n",
    "    px, py = numpy.linspace(0, 1, 1000), []  # for plotting\n",
    "    for ci, c in enumerate(unique_classes):\n",
    "        i = pred_cls == c\n",
    "        nl = nt[ci]  # number of labels\n",
    "        no = i.sum()  # number of outputs\n",
    "        if no == 0 or nl == 0:\n",
    "            continue\n",
    "\n",
    "        # Accumulate FPs and TPs\n",
    "        fpc = (1 - tp[i]).cumsum(0)\n",
    "        tpc = tp[i].cumsum(0)\n",
    "\n",
    "        # Recall\n",
    "        recall = tpc / (nl + eps)  # recall curve\n",
    "        # negative x, xp because xp decreases\n",
    "        r[ci] = numpy.interp(-px, -conf[i], recall[:, 0], left=0)\n",
    "\n",
    "        # Precision\n",
    "        precision = tpc / (tpc + fpc)  # precision curve\n",
    "        p[ci] = numpy.interp(-px, -conf[i], precision[:, 0], left=1)  # p at pr_score\n",
    "\n",
    "        # AP from recall-precision curve\n",
    "        for j in range(tp.shape[1]):\n",
    "            m_rec = numpy.concatenate(([0.0], recall[:, j], [1.0]))\n",
    "            m_pre = numpy.concatenate(([1.0], precision[:, j], [0.0]))\n",
    "\n",
    "            # Compute the precision envelope\n",
    "            m_pre = numpy.flip(numpy.maximum.accumulate(numpy.flip(m_pre)))\n",
    "\n",
    "            # Integrate area under curve\n",
    "            x = numpy.linspace(0, 1, 101)  # 101-point interp (COCO)\n",
    "            ap[ci, j] = numpy.trapz(numpy.interp(x, m_rec, m_pre), x)  # integrate\n",
    "\n",
    "    # Compute F1 (harmonic mean of precision and recall)\n",
    "    f1 = 2 * p * r / (p + r + eps)\n",
    "\n",
    "    i = smooth(f1.mean(0), 0.1).argmax()  # max F1 index\n",
    "    p, r, f1 = p[:, i], r[:, i], f1[:, i]\n",
    "    tp = (r * nt).round()  # true positives\n",
    "    fp = (tp / (p + eps) - tp).round()  # false positives\n",
    "    ap50, ap = ap[:, 0], ap.mean(1)  # AP@0.5, AP@0.5:0.95\n",
    "    m_pre, m_rec = p.mean(), r.mean()\n",
    "    map50, mean_ap = ap50.mean(), ap.mean()\n",
    "    return tp, fp, m_pre, m_rec, map50, mean_ap\n",
    "\n",
    "\n",
    "def strip_optimizer(filename):\n",
    "    x = torch.load(filename, map_location=torch.device('cpu'))\n",
    "    x['model'].half()  # to FP16\n",
    "    for p in x['model'].parameters():\n",
    "        p.requires_grad = False\n",
    "    torch.save(x, filename)\n",
    "\n",
    "\n",
    "def clip_gradients(model, max_norm=10.0):\n",
    "    parameters = model.parameters()\n",
    "    torch.nn.utils.clip_grad_norm_(parameters, max_norm=max_norm)\n",
    "\n",
    "\n",
    "class EMA:\n",
    "    \"\"\"\n",
    "    Updated Exponential Moving Average (EMA) from https://github.com/rwightman/pytorch-image-models\n",
    "    Keeps a moving average of everything in the model state_dict (parameters and buffers)\n",
    "    For EMA details see https://www.tensorflow.org/api_docs/python/tf/train/ExponentialMovingAverage\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, decay=0.9999, tau=2000, updates=0):\n",
    "        # Create EMA\n",
    "        self.ema = copy.deepcopy(model).eval()  # FP32 EMA\n",
    "        self.updates = updates  # number of EMA updates\n",
    "        # decay exponential ramp (to help early epochs)\n",
    "        self.decay = lambda x: decay * (1 - math.exp(-x / tau))\n",
    "        for p in self.ema.parameters():\n",
    "            p.requires_grad_(False)\n",
    "\n",
    "    def update(self, model):\n",
    "        if hasattr(model, 'module'):\n",
    "            model = model.module\n",
    "        # Update EMA parameters\n",
    "        with torch.no_grad():\n",
    "            self.updates += 1\n",
    "            d = self.decay(self.updates)\n",
    "\n",
    "            msd = model.state_dict()  # model state_dict\n",
    "            for k, v in self.ema.state_dict().items():\n",
    "                if v.dtype.is_floating_point:\n",
    "                    v *= d\n",
    "                    v += (1 - d) * msd[k].detach()\n",
    "\n",
    "\n",
    "class AverageMeter:\n",
    "    def __init__(self):\n",
    "        self.num = 0\n",
    "        self.sum = 0\n",
    "        self.avg = 0\n",
    "\n",
    "    def update(self, v, n):\n",
    "        if not math.isnan(float(v)):\n",
    "            self.num = self.num + n\n",
    "            self.sum = self.sum + v * n\n",
    "            self.avg = self.sum / self.num\n",
    "\n",
    "\n",
    "class ComputeLoss:\n",
    "    def __init__(self, model, params):\n",
    "        super().__init__()\n",
    "        if hasattr(model, 'module'):\n",
    "            model = model.module\n",
    "\n",
    "        device = next(model.parameters()).device  # get model device\n",
    "\n",
    "        m = model.head  # Head() module\n",
    "        self.bce = torch.nn.BCEWithLogitsLoss(reduction='none')\n",
    "        self.stride = m.stride  # model strides\n",
    "        self.nc = m.nc  # number of classes\n",
    "        self.no = m.no\n",
    "        self.device = device\n",
    "        self.params = params\n",
    "\n",
    "        # task aligned assigner\n",
    "        self.top_k = 10\n",
    "        self.alpha = 0.5\n",
    "        self.beta = 6.0\n",
    "        self.eps = 1e-9\n",
    "\n",
    "        self.bs = 1\n",
    "        self.num_max_boxes = 0\n",
    "        # DFL Loss params\n",
    "        self.dfl_ch = m.dfl.ch\n",
    "        self.project = torch.arange(self.dfl_ch, dtype=torch.float, device=device)\n",
    "\n",
    "    def __call__(self, outputs, targets):\n",
    "        x = outputs[1] if isinstance(outputs, tuple) else outputs\n",
    "        output = torch.cat([i.view(x[0].shape[0], self.no, -1) for i in x], 2)\n",
    "        pred_output, pred_scores = output.split((4 * self.dfl_ch, self.nc), 1)\n",
    "\n",
    "        pred_output = pred_output.permute(0, 2, 1).contiguous()\n",
    "        pred_scores = pred_scores.permute(0, 2, 1).contiguous()\n",
    "\n",
    "        size = torch.tensor(x[0].shape[2:], dtype=pred_scores.dtype, device=self.device)\n",
    "        size = size * self.stride[0]\n",
    "\n",
    "        anchor_points, stride_tensor = make_anchors(x, self.stride, 0.5)\n",
    "\n",
    "        # targets\n",
    "        if targets.shape[0] == 0:\n",
    "            gt = torch.zeros(pred_scores.shape[0], 0, 5, device=self.device)\n",
    "        else:\n",
    "            i = targets[:, 0]  # image index\n",
    "            _, counts = i.unique(return_counts=True)\n",
    "            gt = torch.zeros(pred_scores.shape[0], counts.max(), 5, device=self.device)\n",
    "            for j in range(pred_scores.shape[0]):\n",
    "                matches = i == j\n",
    "                n = matches.sum()\n",
    "                if n:\n",
    "                    gt[j, :n] = targets[matches, 1:]\n",
    "            gt[..., 1:5] = wh2xy_(gt[..., 1:5].mul_(size[[1, 0, 1, 0]]))\n",
    "\n",
    "        gt_labels, gt_bboxes = gt.split((1, 4), 2)  # cls, xyxy\n",
    "        mask_gt = gt_bboxes.sum(2, keepdim=True).gt_(0)\n",
    "\n",
    "        # boxes\n",
    "        b, a, c = pred_output.shape\n",
    "        pred_bboxes = pred_output.view(b, a, 4, c // 4).softmax(3)\n",
    "        pred_bboxes = pred_bboxes.matmul(self.project.type(pred_bboxes.dtype))\n",
    "\n",
    "        a, b = torch.split(pred_bboxes, 2, -1)\n",
    "        pred_bboxes = torch.cat((anchor_points - a, anchor_points + b), -1)\n",
    "\n",
    "        scores = pred_scores.detach().sigmoid()\n",
    "        bboxes = (pred_bboxes.detach() * stride_tensor).type(gt_bboxes.dtype)\n",
    "        target_bboxes, target_scores, fg_mask = self.assign(scores, bboxes,\n",
    "                                                            gt_labels, gt_bboxes, mask_gt,\n",
    "                                                            anchor_points * stride_tensor)\n",
    "\n",
    "        target_bboxes /= stride_tensor\n",
    "        target_scores_sum = target_scores.sum()\n",
    "\n",
    "        # cls loss\n",
    "        loss_cls = self.bce(pred_scores, target_scores.to(pred_scores.dtype))\n",
    "        loss_cls = loss_cls.sum() / target_scores_sum\n",
    "\n",
    "        # box loss\n",
    "        loss_box = torch.zeros(1, device=self.device)\n",
    "        loss_dfl = torch.zeros(1, device=self.device)\n",
    "        if fg_mask.sum():\n",
    "            # IoU loss\n",
    "            weight = torch.masked_select(target_scores.sum(-1), fg_mask).unsqueeze(-1)\n",
    "            loss_box = self.iou(pred_bboxes[fg_mask], target_bboxes[fg_mask])\n",
    "            loss_box = ((1.0 - loss_box) * weight).sum() / target_scores_sum\n",
    "            # DFL loss\n",
    "            a, b = torch.split(target_bboxes, 2, -1)\n",
    "            target_lt_rb = torch.cat((anchor_points - a, b - anchor_points), -1)\n",
    "            target_lt_rb = target_lt_rb.clamp(0, self.dfl_ch - 1.01)  # distance (left_top, right_bottom)\n",
    "            loss_dfl = self.df_loss(pred_output[fg_mask].view(-1, self.dfl_ch), target_lt_rb[fg_mask])\n",
    "            loss_dfl = (loss_dfl * weight).sum() / target_scores_sum\n",
    "\n",
    "        loss_cls *= self.params['cls']\n",
    "        loss_box *= self.params['box']\n",
    "        loss_dfl *= self.params['dfl']\n",
    "        return loss_cls + loss_box + loss_dfl  # loss(cls, box, dfl)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def assign(self, pred_scores, pred_bboxes, true_labels, true_bboxes, true_mask, anchors):\n",
    "        \"\"\"\n",
    "        Task-aligned One-stage Object Detection assigner\n",
    "        \"\"\"\n",
    "        self.bs = pred_scores.size(0)\n",
    "        self.num_max_boxes = true_bboxes.size(1)\n",
    "\n",
    "        # if self.num_max_boxes == 0:\n",
    "        #     device = true_bboxes.device\n",
    "        #     return (torch.full_like(pred_scores[..., 0], self.nc).to(device),\n",
    "        #             torch.zeros_like(pred_bboxes).to(device),\n",
    "        #             torch.zeros_like(pred_scores).to(device),\n",
    "        #             torch.zeros_like(pred_scores[..., 0]).to(device),\n",
    "        #             torch.zeros_like(pred_scores[..., 0]).to(device))\n",
    "        if self.num_max_boxes == 0:\n",
    "            device = true_bboxes.device\n",
    "            target_bboxes = torch.zeros_like(pred_bboxes).to(device)\n",
    "            target_scores = torch.zeros_like(pred_scores).to(device)\n",
    "            fg_mask = torch.zeros_like(pred_scores[..., 0]).to(device)\n",
    "            return target_bboxes, target_scores, fg_mask\n",
    "\n",
    "        i = torch.zeros([2, self.bs, self.num_max_boxes], dtype=torch.long)\n",
    "        i[0] = torch.arange(end=self.bs).view(-1, 1).repeat(1, self.num_max_boxes)\n",
    "        i[1] = true_labels.long().squeeze(-1)\n",
    "\n",
    "        overlaps = self.iou(true_bboxes.unsqueeze(2), pred_bboxes.unsqueeze(1))\n",
    "        overlaps = overlaps.squeeze(3).clamp(0)\n",
    "        align_metric = pred_scores[i[0], :, i[1]].pow(self.alpha) * overlaps.pow(self.beta)\n",
    "        bs, n_boxes, _ = true_bboxes.shape\n",
    "        lt, rb = true_bboxes.view(-1, 1, 4).chunk(2, 2)  # left-top, right-bottom\n",
    "        bbox_deltas = torch.cat((anchors[None] - lt, rb - anchors[None]), dim=2)\n",
    "        mask_in_gts = bbox_deltas.view(bs, n_boxes, anchors.shape[0], -1).amin(3).gt_(1e-9)\n",
    "        metrics = align_metric * mask_in_gts\n",
    "        top_k_mask = true_mask.repeat([1, 1, self.top_k]).bool()\n",
    "        num_anchors = metrics.shape[-1]\n",
    "        top_k_metrics, top_k_indices = torch.topk(metrics, self.top_k, dim=-1, largest=True)\n",
    "        if top_k_mask is None:\n",
    "            top_k_mask = (top_k_metrics.max(-1, keepdim=True) > self.eps).tile([1, 1, self.top_k])\n",
    "        top_k_indices = torch.where(top_k_mask, top_k_indices, 0)\n",
    "        is_in_top_k = one_hot(top_k_indices, num_anchors).sum(-2)\n",
    "        # filter invalid boxes\n",
    "        is_in_top_k = torch.where(is_in_top_k > 1, 0, is_in_top_k)\n",
    "        mask_top_k = is_in_top_k.to(metrics.dtype)\n",
    "        # merge all mask to a final mask, (b, max_num_obj, h*w)\n",
    "        mask_pos = mask_top_k * mask_in_gts * true_mask\n",
    "\n",
    "        fg_mask = mask_pos.sum(-2)\n",
    "        if fg_mask.max() > 1:  # one anchor is assigned to multiple gt_bboxes\n",
    "            mask_multi_gts = (fg_mask.unsqueeze(1) > 1).repeat([1, self.num_max_boxes, 1])\n",
    "            max_overlaps_idx = overlaps.argmax(1)\n",
    "            is_max_overlaps = one_hot(max_overlaps_idx, self.num_max_boxes)\n",
    "            is_max_overlaps = is_max_overlaps.permute(0, 2, 1).to(overlaps.dtype)\n",
    "            mask_pos = torch.where(mask_multi_gts, is_max_overlaps, mask_pos)\n",
    "            fg_mask = mask_pos.sum(-2)\n",
    "        # find each grid serve which gt(index)\n",
    "        target_gt_idx = mask_pos.argmax(-2)  # (b, h*w)\n",
    "\n",
    "        # assigned target labels, (b, 1)\n",
    "        batch_index = torch.arange(end=self.bs,\n",
    "                                   dtype=torch.int64,\n",
    "                                   device=true_labels.device)[..., None]\n",
    "        target_gt_idx = target_gt_idx + batch_index * self.num_max_boxes\n",
    "        target_labels = true_labels.long().flatten()[target_gt_idx]\n",
    "\n",
    "        # assigned target boxes\n",
    "        target_bboxes = true_bboxes.view(-1, 4)[target_gt_idx]\n",
    "\n",
    "        # assigned target scores\n",
    "        target_labels.clamp(0)\n",
    "        target_scores = one_hot(target_labels, self.nc)\n",
    "        fg_scores_mask = fg_mask[:, :, None].repeat(1, 1, self.nc)\n",
    "        target_scores = torch.where(fg_scores_mask > 0, target_scores, 0)\n",
    "\n",
    "        # normalize\n",
    "        align_metric *= mask_pos\n",
    "        pos_align_metrics = align_metric.amax(axis=-1, keepdim=True)\n",
    "        pos_overlaps = (overlaps * mask_pos).amax(axis=-1, keepdim=True)\n",
    "        norm_align_metric = (align_metric * pos_overlaps / (pos_align_metrics + self.eps)).amax(-2)\n",
    "        norm_align_metric = norm_align_metric.unsqueeze(-1)\n",
    "        target_scores = target_scores * norm_align_metric\n",
    "\n",
    "        return target_bboxes, target_scores, fg_mask.bool()\n",
    "\n",
    "    @staticmethod\n",
    "    def df_loss(pred_dist, target):\n",
    "        # Return sum of left and right DFL losses\n",
    "        # Distribution Focal Loss https://ieeexplore.ieee.org/document/9792391\n",
    "        tl = target.long()  # target left\n",
    "        tr = tl + 1  # target right\n",
    "        wl = tr - target  # weight left\n",
    "        wr = 1 - wl  # weight right\n",
    "        l_loss = cross_entropy(pred_dist, tl.view(-1), reduction=\"none\").view(tl.shape)\n",
    "        r_loss = cross_entropy(pred_dist, tr.view(-1), reduction=\"none\").view(tl.shape)\n",
    "        return (l_loss * wl + r_loss * wr).mean(-1, keepdim=True)\n",
    "\n",
    "    @staticmethod\n",
    "    def iou(box1, box2, eps=1e-7):\n",
    "        # Returns Intersection over Union (IoU) of box1(1,4) to box2(n,4)\n",
    "\n",
    "        # Get the coordinates of bounding boxes\n",
    "        b1_x1, b1_y1, b1_x2, b1_y2 = box1.chunk(4, -1)\n",
    "        b2_x1, b2_y1, b2_x2, b2_y2 = box2.chunk(4, -1)\n",
    "        w1, h1 = b1_x2 - b1_x1, b1_y2 - b1_y1 + eps\n",
    "        w2, h2 = b2_x2 - b2_x1, b2_y2 - b2_y1 + eps\n",
    "\n",
    "        # Intersection area\n",
    "        area1 = b1_x2.minimum(b2_x2) - b1_x1.maximum(b2_x1)\n",
    "        area2 = b1_y2.minimum(b2_y2) - b1_y1.maximum(b2_y1)\n",
    "        intersection = area1.clamp(0) * area2.clamp(0)\n",
    "\n",
    "        # Union Area\n",
    "        union = w1 * h1 + w2 * h2 - intersection + eps\n",
    "\n",
    "        # IoU\n",
    "        iou = intersection / union\n",
    "        cw = b1_x2.maximum(b2_x2) - b1_x1.minimum(b2_x1)  # convex width\n",
    "        ch = b1_y2.maximum(b2_y2) - b1_y1.minimum(b2_y1)  # convex height\n",
    "        # Complete IoU https://arxiv.org/abs/1911.08287v1\n",
    "        c2 = cw ** 2 + ch ** 2 + eps  # convex diagonal squared\n",
    "        # center dist ** 2\n",
    "        rho2 = ((b2_x1 + b2_x2 - b1_x1 - b1_x2) ** 2 + (b2_y1 + b2_y2 - b1_y1 - b1_y2) ** 2) / 4\n",
    "        # https://github.com/Zzh-tju/DIoU-SSD-pytorch/blob/master/utils/box/box_utils.py#L47\n",
    "        v = (4 / math.pi ** 2) * (torch.atan(w2 / h2) - torch.atan(w1 / h1)).pow(2)\n",
    "        with torch.no_grad():\n",
    "            alpha = v / (v - iou + (1 + eps))\n",
    "        return iou - (rho2 / c2 + v * alpha)  # CIoU\n",
    "\n",
    "\n",
    "def tensor_to_image(tensor):\n",
    "    tensor = tensor.squeeze().cpu().numpy()\n",
    "    # tensor = (tensor - tensor.min()) / (tensor.max() - tensor.min())\n",
    "    return (tensor * 255).astype(numpy.uint8)\n",
    "\n",
    "\n",
    "\n",
    "# 커스텀 스케줄러 정의\n",
    "class CosineAnnealingWarmUpRestarts(_LRScheduler):\n",
    "    def __init__(self, optimizer, T_0, T_mult=1, eta_max=0.1, T_up=0, gamma=1., last_epoch=-1):\n",
    "        if T_0 <= 0 or not isinstance(T_0, int):\n",
    "            raise ValueError(\"Expected positive integer T_0, but got {}\".format(T_0))\n",
    "        if T_mult < 1 or not isinstance(T_mult, int):\n",
    "            raise ValueError(\"Expected integer T_mult >= 1, but got {}\".format(T_mult))\n",
    "        if T_up < 0 or not isinstance(T_up, int):\n",
    "            raise ValueError(\"Expected positive integer T_up, but got {}\".format(T_up))\n",
    "        \n",
    "        self.T_0 = T_0\n",
    "        self.T_mult = T_mult\n",
    "        self.T_up = T_up\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        self.T_i = T_0\n",
    "        self.cycle = 0\n",
    "        self.T_cur = last_epoch\n",
    "        \n",
    "        # Support multiple eta_max for multiple param groups\n",
    "        if isinstance(eta_max, list) or isinstance(eta_max, tuple):\n",
    "            if len(eta_max) != len(optimizer.param_groups):\n",
    "                raise ValueError(\"Expected len(eta_max) == len(optimizer.param_groups), but got {} and {}\".format(len(eta_max), len(optimizer.param_groups)))\n",
    "            self.eta_max = list(eta_max)\n",
    "        else:\n",
    "            self.eta_max = [eta_max] * len(optimizer.param_groups)\n",
    "        \n",
    "        # Store base_eta_max for scaling\n",
    "        self.base_eta_max = self.eta_max.copy()\n",
    "        \n",
    "        super(CosineAnnealingWarmUpRestarts, self).__init__(optimizer, last_epoch)\n",
    "    \n",
    "    def get_lr(self):\n",
    "        if self.T_cur == -1:\n",
    "            return self.base_lrs\n",
    "        elif self.T_cur < self.T_up:\n",
    "            return [\n",
    "                (eta_max - base_lr) * self.T_cur / self.T_up + base_lr \n",
    "                for base_lr, eta_max in zip(self.base_lrs, self.eta_max)\n",
    "            ]\n",
    "        else:\n",
    "            return [\n",
    "                base_lr + (eta_max - base_lr) * (1 + math.cos(math.pi * (self.T_cur - self.T_up) / (self.T_i - self.T_up))) / 2\n",
    "                for base_lr, eta_max in zip(self.base_lrs, self.eta_max)\n",
    "            ]\n",
    "    \n",
    "    def step(self, epoch=None):\n",
    "        if epoch is None:\n",
    "            epoch = self.last_epoch + 1\n",
    "            self.T_cur = self.T_cur + 1\n",
    "            if self.T_cur >= self.T_i:\n",
    "                self.cycle += 1\n",
    "                self.T_cur = self.T_cur - self.T_i\n",
    "                self.T_i = (self.T_i - self.T_up) * self.T_mult + self.T_up\n",
    "        else:\n",
    "            if epoch >= self.T_0:\n",
    "                if self.T_mult == 1:\n",
    "                    self.T_cur = epoch % self.T_0\n",
    "                    self.cycle = epoch // self.T_0\n",
    "                else:\n",
    "                    n = int(math.log((epoch / self.T_0 * (self.T_mult - 1) + 1), self.T_mult))\n",
    "                    self.cycle = n\n",
    "                    self.T_cur = epoch - self.T_0 * (self.T_mult ** n - 1) / (self.T_mult - 1)\n",
    "                    self.T_i = self.T_0 * self.T_mult ** n\n",
    "            else:\n",
    "                self.T_i = self.T_0\n",
    "                self.T_cur = epoch\n",
    "            \n",
    "        # Update eta_max for each param group\n",
    "        self.eta_max = [base_eta_max * (self.gamma ** self.cycle) for base_eta_max in self.base_eta_max]\n",
    "        self.last_epoch = math.floor(epoch)\n",
    "        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n",
    "            param_group['lr'] = lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "def pad(k, p=None, d=1):\n",
    "    if d > 1:\n",
    "        k = d * (k - 1) + 1\n",
    "    if p is None:\n",
    "        p = k // 2\n",
    "    return p\n",
    "\n",
    "\n",
    "def fuse_conv(conv, norm):\n",
    "    fused_conv = torch.nn.Conv2d(conv.in_channels,\n",
    "                                 conv.out_channels,\n",
    "                                 kernel_size=conv.kernel_size,\n",
    "                                 stride=conv.stride,\n",
    "                                 padding=conv.padding,\n",
    "                                 groups=conv.groups,\n",
    "                                 bias=True).requires_grad_(False).to(conv.weight.device)\n",
    "\n",
    "    w_conv = conv.weight.clone().view(conv.out_channels, -1)\n",
    "    w_norm = torch.diag(norm.weight.div(torch.sqrt(norm.eps + norm.running_var)))\n",
    "    fused_conv.weight.copy_(torch.mm(w_norm, w_conv).view(fused_conv.weight.size()))\n",
    "\n",
    "    b_conv = torch.zeros(conv.weight.size(0), device=conv.weight.device) if conv.bias is None else conv.bias\n",
    "    b_norm = norm.bias - norm.weight.mul(norm.running_mean).div(torch.sqrt(norm.running_var + norm.eps))\n",
    "    fused_conv.bias.copy_(torch.mm(w_norm, b_conv.reshape(-1, 1)).reshape(-1) + b_norm)\n",
    "\n",
    "    return fused_conv\n",
    "\n",
    "\n",
    "class Conv(torch.nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, k=1, s=1, p=None, d=1, g=1):\n",
    "        super().__init__()\n",
    "        self.conv = torch.nn.Conv2d(in_ch, out_ch, k, s, pad(k, p, d), d, g, False)\n",
    "        self.norm = torch.nn.BatchNorm2d(out_ch, 0.001, 0.03)\n",
    "        self.relu = torch.nn.SiLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.relu(self.norm(self.conv(x)))\n",
    "\n",
    "    def fuse_forward(self, x):\n",
    "        return self.relu(self.conv(x))\n",
    "\n",
    "\n",
    "class Residual(torch.nn.Module):\n",
    "    def __init__(self, ch, add=True):\n",
    "        super().__init__()\n",
    "        self.add_m = add\n",
    "        self.res_m = torch.nn.Sequential(Conv(ch, ch, 3),\n",
    "                                         Conv(ch, ch, 3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.res_m(x) + x if self.add_m else self.res_m(x)\n",
    "\n",
    "\n",
    "class CSP(torch.nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, n=1, add=True):\n",
    "        super().__init__()\n",
    "        self.conv1 = Conv(in_ch, out_ch // 2)\n",
    "        self.conv2 = Conv(in_ch, out_ch // 2)\n",
    "        self.conv3 = Conv((2 + n) * out_ch // 2, out_ch)\n",
    "        self.res_m = torch.nn.ModuleList(Residual(out_ch // 2, add) for _ in range(n))\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = [self.conv1(x), self.conv2(x)]\n",
    "        y.extend(m(y[-1]) for m in self.res_m)\n",
    "        return self.conv3(torch.cat(y, dim=1))\n",
    "\n",
    "\n",
    "class SPP(torch.nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, k=5):\n",
    "        super().__init__()\n",
    "        self.conv1 = Conv(in_ch, in_ch // 2)\n",
    "        self.conv2 = Conv(in_ch * 2, out_ch)\n",
    "        self.res_m = torch.nn.MaxPool2d(k, 1, k // 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        y1 = self.res_m(x)\n",
    "        y2 = self.res_m(y1)\n",
    "        return self.conv2(torch.cat([x, y1, y2, self.res_m(y2)], 1))\n",
    "\n",
    "\n",
    "class DarkNet(torch.nn.Module):\n",
    "    def __init__(self, width, depth):\n",
    "        super().__init__()\n",
    "        p1 = [Conv(width[0], width[1], 3, 2)]\n",
    "        p2 = [Conv(width[1], width[2], 3, 2),\n",
    "              CSP(width[2], width[2], depth[0])]\n",
    "        p3 = [Conv(width[2], width[3], 3, 2),\n",
    "              CSP(width[3], width[3], depth[1])]\n",
    "        p4 = [Conv(width[3], width[4], 3, 2),\n",
    "              CSP(width[4], width[4], depth[2])]\n",
    "        p5 = [Conv(width[4], width[5], 3, 2),\n",
    "              CSP(width[5], width[5], depth[0]),\n",
    "              SPP(width[5], width[5])]\n",
    "\n",
    "        self.p1 = torch.nn.Sequential(*p1)\n",
    "        self.p2 = torch.nn.Sequential(*p2)\n",
    "        self.p3 = torch.nn.Sequential(*p3)\n",
    "        self.p4 = torch.nn.Sequential(*p4)\n",
    "        self.p5 = torch.nn.Sequential(*p5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        p1 = self.p1(x)\n",
    "        p2 = self.p2(p1)\n",
    "        p3 = self.p3(p2)\n",
    "        p4 = self.p4(p3)\n",
    "        p5 = self.p5(p4)\n",
    "        return p3, p4, p5\n",
    "\n",
    "\n",
    "class DarkFPN(torch.nn.Module):\n",
    "    def __init__(self, width, depth):\n",
    "        super().__init__()\n",
    "        self.up = torch.nn.Upsample(None, 2)\n",
    "        self.h1 = CSP(width[4] + width[5], width[4], depth[0], False)\n",
    "        self.h2 = CSP(width[3] + width[4], width[3], depth[0], False)\n",
    "        self.h3 = Conv(width[3], width[3], 3, 2)\n",
    "        self.h4 = CSP(width[3] + width[4], width[4], depth[0], False)\n",
    "        self.h5 = Conv(width[4], width[4], 3, 2)\n",
    "        self.h6 = CSP(width[4] + width[5], width[5], depth[0], False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        p3, p4, p5 = x\n",
    "        h1 = self.h1(torch.cat([self.up(p5), p4], 1))\n",
    "        h2 = self.h2(torch.cat([self.up(h1), p3], 1))\n",
    "        h4 = self.h4(torch.cat([self.h3(h2), h1], 1))\n",
    "        h6 = self.h6(torch.cat([self.h5(h4), p5], 1))\n",
    "        return h2, h4, h6\n",
    "\n",
    "\n",
    "class DFL(torch.nn.Module):\n",
    "    # Integral module of Distribution Focal Loss (DFL)\n",
    "    # Generalized Focal Loss https://ieeexplore.ieee.org/document/9792391\n",
    "    def __init__(self, ch=16):\n",
    "        super().__init__()\n",
    "        self.ch = ch\n",
    "        self.conv = torch.nn.Conv2d(ch, 1, 1, bias=False).requires_grad_(False)\n",
    "        x = torch.arange(ch, dtype=torch.float).view(1, ch, 1, 1)\n",
    "        self.conv.weight.data[:] = torch.nn.Parameter(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, a = x.shape\n",
    "        x = x.view(b, 4, self.ch, a).transpose(2, 1)\n",
    "        return self.conv(x.softmax(1)).view(b, 4, a)\n",
    "\n",
    "\n",
    "class Head(torch.nn.Module):\n",
    "    anchors = torch.empty(0)\n",
    "    strides = torch.empty(0)\n",
    "\n",
    "    def __init__(self, nc=80, filters=()):\n",
    "        super().__init__()\n",
    "        self.ch = 16  # DFL channels\n",
    "        self.nc = nc  # number of classes\n",
    "        self.nl = len(filters)  # number of detection layers\n",
    "        self.no = nc + self.ch * 4  # number of outputs per anchor\n",
    "        self.stride = torch.zeros(self.nl)  # strides computed during build\n",
    "\n",
    "        c1 = max(filters[0], self.nc)\n",
    "        c2 = max((filters[0] // 4, self.ch * 4))\n",
    "\n",
    "        self.dfl = DFL(self.ch)\n",
    "        self.cls = torch.nn.ModuleList(torch.nn.Sequential(Conv(x, c1, 3),\n",
    "                                                           Conv(c1, c1, 3),\n",
    "                                                           torch.nn.Conv2d(c1, self.nc, 1)) for x in filters)\n",
    "        self.box = torch.nn.ModuleList(torch.nn.Sequential(Conv(x, c2, 3),\n",
    "                                                           Conv(c2, c2, 3),\n",
    "                                                           torch.nn.Conv2d(c2, 4 * self.ch, 1)) for x in filters)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i in range(self.nl):\n",
    "            x[i] = torch.cat((self.box[i](x[i]), self.cls[i](x[i])), 1)\n",
    "        if self.training:\n",
    "            return x\n",
    "        self.anchors, self.strides = (x.transpose(0, 1) for x in make_anchors(x, self.stride, 0.5))\n",
    "\n",
    "        x = torch.cat([i.view(x[0].shape[0], self.no, -1) for i in x], 2)\n",
    "        box, cls = x.split((self.ch * 4, self.nc), 1)\n",
    "        a, b = torch.split(self.dfl(box), 2, 1)\n",
    "        a = self.anchors.unsqueeze(0) - a\n",
    "        b = self.anchors.unsqueeze(0) + b\n",
    "        box = torch.cat(((a + b) / 2, b - a), 1)\n",
    "        return torch.cat((box * self.strides, cls.sigmoid()), 1)\n",
    "\n",
    "    def initialize_biases(self):\n",
    "        # Initialize biases\n",
    "        # WARNING: requires stride availability\n",
    "        m = self\n",
    "        for a, b, s in zip(m.box, m.cls, m.stride):\n",
    "            a[-1].bias.data[:] = 1.0  # box\n",
    "            # cls (.01 objects, 80 classes, 640 img)\n",
    "            b[-1].bias.data[:m.nc] = math.log(5 / m.nc / (640 / s) ** 2)\n",
    "\n",
    "\n",
    "class YOLO(torch.nn.Module):\n",
    "    def __init__(self, width, depth, num_classes):\n",
    "        super().__init__()\n",
    "        self.net = DarkNet(width, depth)\n",
    "        self.fpn = DarkFPN(width, depth)\n",
    "\n",
    "        img_dummy = torch.zeros(1, 3, 256, 256)\n",
    "        self.head = Head(num_classes, (width[3], width[4], width[5]))\n",
    "        self.head.stride = torch.tensor([256 / x.shape[-2] for x in self.forward(img_dummy)])\n",
    "        self.stride = self.head.stride\n",
    "        self.head.initialize_biases()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        x = self.fpn(x)\n",
    "        return self.head(list(x))\n",
    "\n",
    "    def fuse(self):\n",
    "        for m in self.modules():\n",
    "            if type(m) is Conv and hasattr(m, 'norm'):\n",
    "                m.conv = fuse_conv(m.conv, m.norm)\n",
    "                m.forward = m.fuse_forward\n",
    "                delattr(m, 'norm')\n",
    "        return self\n",
    "\n",
    "\n",
    "def yolo_v8_n(num_classes: int = 80):\n",
    "    depth = [1, 2, 2]\n",
    "    width = [3, 16, 32, 64, 128, 256]\n",
    "    return YOLO(width, depth, num_classes)\n",
    "\n",
    "\n",
    "def yolo_v8_s(num_classes: int = 80):\n",
    "    depth = [1, 2, 2]\n",
    "    width = [3, 32, 64, 128, 256, 512]\n",
    "    return YOLO(width, depth, num_classes)\n",
    "\n",
    "\n",
    "def yolo_v8_m(num_classes: int = 80):\n",
    "    depth = [2, 4, 4]\n",
    "    width = [3, 48, 96, 192, 384, 576]\n",
    "    return YOLO(width, depth, num_classes)\n",
    "\n",
    "\n",
    "def yolo_v8_l(num_classes: int = 80):\n",
    "    depth = [3, 6, 6]\n",
    "    width = [3, 64, 128, 256, 512, 512]\n",
    "    return YOLO(width, depth, num_classes)\n",
    "\n",
    "\n",
    "def yolo_v8_x(num_classes: int = 80):\n",
    "    depth = [3, 6, 6]\n",
    "    width = [3, 80, 160, 320, 640, 640]\n",
    "    return YOLO(width, depth, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import random\n",
    "\n",
    "import cv2\n",
    "import numpy\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils import data\n",
    "\n",
    "FORMATS = 'bmp', 'dng', 'jpeg', 'jpg', 'mpo', 'png', 'tif', 'tiff', 'webp'\n",
    "\n",
    "\n",
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, filenames, input_size, params, augment):\n",
    "        self.params = params\n",
    "        self.mosaic = augment\n",
    "        self.augment = augment\n",
    "        self.input_size = input_size\n",
    "\n",
    "        # Read labels\n",
    "        cache = self.load_label(filenames)\n",
    "        labels, shapes = zip(*cache.values())\n",
    "        self.labels = list(labels)\n",
    "        self.shapes = numpy.array(shapes, dtype=numpy.float64)\n",
    "        self.filenames = list(cache.keys())  # update\n",
    "        self.n = len(shapes)  # number of samples\n",
    "        self.indices = range(self.n)\n",
    "        # Albumentations (optional, only used if package is installed)\n",
    "        self.albumentations = Albumentations()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        index = self.indices[index]\n",
    "\n",
    "        params = self.params\n",
    "        mosaic = self.mosaic and random.random() < params['mosaic']\n",
    "\n",
    "        if mosaic:\n",
    "            shapes = None\n",
    "            # Load MOSAIC\n",
    "            image, label = self.load_mosaic(index, params)\n",
    "            # MixUp augmentation\n",
    "            if random.random() < params['mix_up']:\n",
    "                index = random.choice(self.indices)\n",
    "                mix_image1, mix_label1 = image, label\n",
    "                mix_image2, mix_label2 = self.load_mosaic(index, params)\n",
    "\n",
    "                image, label = mix_up(mix_image1, mix_label1, mix_image2, mix_label2)\n",
    "        else:\n",
    "            # Load image\n",
    "            image, shape = self.load_image(index)\n",
    "            h, w = image.shape[:2]\n",
    "\n",
    "            # Resize\n",
    "            image, ratio, pad = resize(image, self.input_size, self.augment)\n",
    "            shapes = shape, ((h / shape[0], w / shape[1]), pad)  # for COCO mAP rescaling\n",
    "\n",
    "            label = self.labels[index].copy()\n",
    "            if label.size:\n",
    "                label[:, 1:] = wh2xy(label[:, 1:], ratio[0] * w, ratio[1] * h, pad[0], pad[1])\n",
    "            if self.augment:\n",
    "                image, label = random_perspective(image, label, params)\n",
    "        nl = len(label)  # number of labels\n",
    "        if nl:\n",
    "            label[:, 1:5] = xy2wh(label[:, 1:5], image.shape[1], image.shape[0])\n",
    "\n",
    "        if self.augment:\n",
    "            # Albumentations\n",
    "            image, label = self.albumentations(image, label)\n",
    "            nl = len(label)  # update after albumentations\n",
    "            # HSV color-space\n",
    "            augment_hsv(image, params)\n",
    "            # Flip up-down\n",
    "            if random.random() < params['flip_ud']:\n",
    "                image = numpy.flipud(image)\n",
    "                if nl:\n",
    "                    label[:, 2] = 1 - label[:, 2]\n",
    "            # Flip left-right\n",
    "            if random.random() < params['flip_lr']:\n",
    "                image = numpy.fliplr(image)\n",
    "                if nl:\n",
    "                    label[:, 1] = 1 - label[:, 1]\n",
    "\n",
    "        target = torch.zeros((nl, 6))\n",
    "        if nl:\n",
    "            target[:, 1:] = torch.from_numpy(label)\n",
    "\n",
    "        # Convert HWC to CHW, BGR to RGB\n",
    "        sample = image.transpose((2, 0, 1))[::-1]\n",
    "        sample = numpy.ascontiguousarray(sample)\n",
    "\n",
    "        return torch.from_numpy(sample), target, shapes\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def load_image(self, i):\n",
    "        image = cv2.imread(self.filenames[i])\n",
    "        h, w = image.shape[:2]\n",
    "        r = self.input_size / max(h, w)\n",
    "        if r != 1:\n",
    "            image = cv2.resize(image,\n",
    "                               dsize=(int(w * r), int(h * r)),\n",
    "                               interpolation=resample() if self.augment else cv2.INTER_LINEAR)\n",
    "        return image, (h, w)\n",
    "\n",
    "    def load_mosaic(self, index, params):\n",
    "        label4 = []\n",
    "        image4 = numpy.full((self.input_size * 2, self.input_size * 2, 3), 0, dtype=numpy.uint8)\n",
    "        y1a, y2a, x1a, x2a, y1b, y2b, x1b, x2b = (None, None, None, None, None, None, None, None)\n",
    "\n",
    "        border = [-self.input_size // 2, -self.input_size // 2]\n",
    "\n",
    "        xc = int(random.uniform(-border[0], 2 * self.input_size + border[1]))\n",
    "        yc = int(random.uniform(-border[0], 2 * self.input_size + border[1]))\n",
    "\n",
    "        indices = [index] + random.choices(self.indices, k=3)\n",
    "        random.shuffle(indices)\n",
    "\n",
    "        for i, index in enumerate(indices):\n",
    "            # Load image\n",
    "            image, _ = self.load_image(index)\n",
    "            shape = image.shape\n",
    "            if i == 0:  # top left\n",
    "                x1a = max(xc - shape[1], 0)\n",
    "                y1a = max(yc - shape[0], 0)\n",
    "                x2a = xc\n",
    "                y2a = yc\n",
    "                x1b = shape[1] - (x2a - x1a)\n",
    "                y1b = shape[0] - (y2a - y1a)\n",
    "                x2b = shape[1]\n",
    "                y2b = shape[0]\n",
    "            if i == 1:  # top right\n",
    "                x1a = xc\n",
    "                y1a = max(yc - shape[0], 0)\n",
    "                x2a = min(xc + shape[1], self.input_size * 2)\n",
    "                y2a = yc\n",
    "                x1b = 0\n",
    "                y1b = shape[0] - (y2a - y1a)\n",
    "                x2b = min(shape[1], x2a - x1a)\n",
    "                y2b = shape[0]\n",
    "            if i == 2:  # bottom left\n",
    "                x1a = max(xc - shape[1], 0)\n",
    "                y1a = yc\n",
    "                x2a = xc\n",
    "                y2a = min(self.input_size * 2, yc + shape[0])\n",
    "                x1b = shape[1] - (x2a - x1a)\n",
    "                y1b = 0\n",
    "                x2b = shape[1]\n",
    "                y2b = min(y2a - y1a, shape[0])\n",
    "            if i == 3:  # bottom right\n",
    "                x1a = xc\n",
    "                y1a = yc\n",
    "                x2a = min(xc + shape[1], self.input_size * 2)\n",
    "                y2a = min(self.input_size * 2, yc + shape[0])\n",
    "                x1b = 0\n",
    "                y1b = 0\n",
    "                x2b = min(shape[1], x2a - x1a)\n",
    "                y2b = min(y2a - y1a, shape[0])\n",
    "\n",
    "            image4[y1a:y2a, x1a:x2a] = image[y1b:y2b, x1b:x2b]\n",
    "            pad_w = x1a - x1b\n",
    "            pad_h = y1a - y1b\n",
    "\n",
    "            # Labels\n",
    "            label = self.labels[index].copy()\n",
    "            if len(label):\n",
    "                label[:, 1:] = wh2xy(label[:, 1:], shape[1], shape[0], pad_w, pad_h)\n",
    "            label4.append(label)\n",
    "\n",
    "        # Concat/clip labels\n",
    "        label4 = numpy.concatenate(label4, 0)\n",
    "        for x in label4[:, 1:]:\n",
    "            numpy.clip(x, 0, 2 * self.input_size, out=x)\n",
    "\n",
    "        # Augment\n",
    "        image4, label4 = random_perspective(image4, label4, params, border)\n",
    "\n",
    "        return image4, label4\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        samples, targets, shapes = zip(*batch)\n",
    "        for i, item in enumerate(targets):\n",
    "            item[:, 0] = i  # add target image index\n",
    "        return torch.stack(samples, 0), torch.cat(targets, 0), shapes\n",
    "\n",
    "    @staticmethod\n",
    "    def load_label(filenames):\n",
    "        path = f'{os.path.dirname(filenames[0])}.cache'\n",
    "        if os.path.exists(path):\n",
    "            return torch.load(path)\n",
    "        x = {}\n",
    "        for filename in filenames:\n",
    "            try:\n",
    "                # verify images\n",
    "                with open(filename, 'rb') as f:\n",
    "                    image = Image.open(f)\n",
    "                    image.verify()  # PIL verify\n",
    "                shape = image.size  # image size\n",
    "                assert (shape[0] > 9) & (shape[1] > 9), f'image size {shape} <10 pixels'\n",
    "                assert image.format.lower() in FORMATS, f'invalid image format {image.format}'\n",
    "\n",
    "                # verify labels\n",
    "                a = f'{os.sep}images{os.sep}'\n",
    "                b = f'{os.sep}labels{os.sep}'\n",
    "                if os.path.isfile(b.join(filename.rsplit(a, 1)).rsplit('.', 1)[0] + '.txt'):\n",
    "                    with open(b.join(filename.rsplit(a, 1)).rsplit('.', 1)[0] + '.txt') as f:\n",
    "                        label = [x.split() for x in f.read().strip().splitlines() if len(x)]\n",
    "                        label = numpy.array(label, dtype=numpy.float32)\n",
    "                    nl = len(label)\n",
    "                    if nl:\n",
    "                        assert label.shape[1] == 5, 'labels require 5 columns'\n",
    "                        assert (label >= 0).all(), 'negative label values'\n",
    "                        assert (label[:, 1:] <= 1).all(), 'non-normalized coordinates'\n",
    "                        _, i = numpy.unique(label, axis=0, return_index=True)\n",
    "                        if len(i) < nl:  # duplicate row check\n",
    "                            label = label[i]  # remove duplicates\n",
    "                    else:\n",
    "                        label = numpy.zeros((0, 5), dtype=numpy.float32)\n",
    "                else:\n",
    "                    label = numpy.zeros((0, 5), dtype=numpy.float32)\n",
    "                if filename:\n",
    "                    x[filename] = [label, shape]\n",
    "            except FileNotFoundError:\n",
    "                pass\n",
    "        torch.save(x, path)\n",
    "        return x\n",
    "\n",
    "\n",
    "def wh2xy(x, w=640, h=640, pad_w=0, pad_h=0):\n",
    "    # Convert nx4 boxes\n",
    "    # from [x, y, w, h] normalized to [x1, y1, x2, y2] where xy1=top-left, xy2=bottom-right\n",
    "    y = numpy.copy(x)\n",
    "    y[:, 0] = w * (x[:, 0] - x[:, 2] / 2) + pad_w  # top left x\n",
    "    y[:, 1] = h * (x[:, 1] - x[:, 3] / 2) + pad_h  # top left y\n",
    "    y[:, 2] = w * (x[:, 0] + x[:, 2] / 2) + pad_w  # bottom right x\n",
    "    y[:, 3] = h * (x[:, 1] + x[:, 3] / 2) + pad_h  # bottom right y\n",
    "    return y\n",
    "\n",
    "\n",
    "def xy2wh(x, w=640, h=640):\n",
    "    # warning: inplace clip\n",
    "    x[:, [0, 2]] = x[:, [0, 2]].clip(0, w - 1E-3)  # x1, x2\n",
    "    x[:, [1, 3]] = x[:, [1, 3]].clip(0, h - 1E-3)  # y1, y2\n",
    "\n",
    "    # Convert nx4 boxes\n",
    "    # from [x1, y1, x2, y2] to [x, y, w, h] normalized where xy1=top-left, xy2=bottom-right\n",
    "    y = numpy.copy(x)\n",
    "    y[:, 0] = ((x[:, 0] + x[:, 2]) / 2) / w  # x center\n",
    "    y[:, 1] = ((x[:, 1] + x[:, 3]) / 2) / h  # y center\n",
    "    y[:, 2] = (x[:, 2] - x[:, 0]) / w  # width\n",
    "    y[:, 3] = (x[:, 3] - x[:, 1]) / h  # height\n",
    "    return y\n",
    "\n",
    "\n",
    "def resample():\n",
    "    choices = (cv2.INTER_AREA,\n",
    "               cv2.INTER_CUBIC,\n",
    "               cv2.INTER_LINEAR,\n",
    "               cv2.INTER_NEAREST,\n",
    "               cv2.INTER_LANCZOS4)\n",
    "    return random.choice(seq=choices)\n",
    "\n",
    "\n",
    "def augment_hsv(image, params):\n",
    "    # HSV color-space augmentation\n",
    "    h = params['hsv_h']\n",
    "    s = params['hsv_s']\n",
    "    v = params['hsv_v']\n",
    "\n",
    "    r = numpy.random.uniform(-1, 1, 3) * [h, s, v] + 1\n",
    "    h, s, v = cv2.split(cv2.cvtColor(image, cv2.COLOR_BGR2HSV))\n",
    "\n",
    "    x = numpy.arange(0, 256, dtype=r.dtype)\n",
    "    lut_h = ((x * r[0]) % 180).astype('uint8')\n",
    "    lut_s = numpy.clip(x * r[1], 0, 255).astype('uint8')\n",
    "    lut_v = numpy.clip(x * r[2], 0, 255).astype('uint8')\n",
    "\n",
    "    im_hsv = cv2.merge((cv2.LUT(h, lut_h), cv2.LUT(s, lut_s), cv2.LUT(v, lut_v)))\n",
    "    cv2.cvtColor(im_hsv, cv2.COLOR_HSV2BGR, dst=image)  # no return needed\n",
    "\n",
    "\n",
    "def resize(image, input_size, augment):\n",
    "    # Resize and pad image while meeting stride-multiple constraints\n",
    "    shape = image.shape[:2]  # current shape [height, width]\n",
    "\n",
    "    # Scale ratio (new / old)\n",
    "    r = min(input_size / shape[0], input_size / shape[1])\n",
    "    if not augment:  # only scale down, do not scale up (for better val mAP)\n",
    "        r = min(r, 1.0)\n",
    "\n",
    "    # Compute padding\n",
    "    pad = int(round(shape[1] * r)), int(round(shape[0] * r))\n",
    "    w = (input_size - pad[0]) / 2\n",
    "    h = (input_size - pad[1]) / 2\n",
    "\n",
    "    if shape[::-1] != pad:  # resize\n",
    "        image = cv2.resize(image,\n",
    "                           dsize=pad,\n",
    "                           interpolation=resample() if augment else cv2.INTER_LINEAR)\n",
    "    top, bottom = int(round(h - 0.1)), int(round(h + 0.1))\n",
    "    left, right = int(round(w - 0.1)), int(round(w + 0.1))\n",
    "    image = cv2.copyMakeBorder(image, top, bottom, left, right, cv2.BORDER_CONSTANT)  # add border\n",
    "    return image, (r, r), (w, h)\n",
    "\n",
    "\n",
    "def candidates(box1, box2):\n",
    "    # box1(4,n), box2(4,n)\n",
    "    w1, h1 = box1[2] - box1[0], box1[3] - box1[1]\n",
    "    w2, h2 = box2[2] - box2[0], box2[3] - box2[1]\n",
    "    aspect_ratio = numpy.maximum(w2 / (h2 + 1e-16), h2 / (w2 + 1e-16))  # aspect ratio\n",
    "    return (w2 > 2) & (h2 > 2) & (w2 * h2 / (w1 * h1 + 1e-16) > 0.1) & (aspect_ratio < 100)\n",
    "\n",
    "\n",
    "def random_perspective(samples, targets, params, border=(0, 0)):\n",
    "    h = samples.shape[0] + border[0] * 2\n",
    "    w = samples.shape[1] + border[1] * 2\n",
    "\n",
    "    # Center\n",
    "    center = numpy.eye(3)\n",
    "    center[0, 2] = -samples.shape[1] / 2  # x translation (pixels)\n",
    "    center[1, 2] = -samples.shape[0] / 2  # y translation (pixels)\n",
    "\n",
    "    # Perspective\n",
    "    perspective = numpy.eye(3)\n",
    "\n",
    "    # Rotation and Scale\n",
    "    rotate = numpy.eye(3)\n",
    "    a = random.uniform(-params['degrees'], params['degrees'])\n",
    "    s = random.uniform(1 - params['scale'], 1 + params['scale'])\n",
    "    rotate[:2] = cv2.getRotationMatrix2D(angle=a, center=(0, 0), scale=s)\n",
    "\n",
    "    # Shear\n",
    "    shear = numpy.eye(3)\n",
    "    shear[0, 1] = math.tan(random.uniform(-params['shear'], params['shear']) * math.pi / 180)\n",
    "    shear[1, 0] = math.tan(random.uniform(-params['shear'], params['shear']) * math.pi / 180)\n",
    "\n",
    "    # Translation\n",
    "    translate = numpy.eye(3)\n",
    "    translate[0, 2] = random.uniform(0.5 - params['translate'], 0.5 + params['translate']) * w\n",
    "    translate[1, 2] = random.uniform(0.5 - params['translate'], 0.5 + params['translate']) * h\n",
    "\n",
    "    # Combined rotation matrix, order of operations (right to left) is IMPORTANT\n",
    "    matrix = translate @ shear @ rotate @ perspective @ center\n",
    "    if (border[0] != 0) or (border[1] != 0) or (matrix != numpy.eye(3)).any():  # image changed\n",
    "        samples = cv2.warpAffine(samples, matrix[:2], dsize=(w, h), borderValue=(0, 0, 0))\n",
    "\n",
    "    # Transform label coordinates\n",
    "    n = len(targets)\n",
    "    if n:\n",
    "        xy = numpy.ones((n * 4, 3))\n",
    "        xy[:, :2] = targets[:, [1, 2, 3, 4, 1, 4, 3, 2]].reshape(n * 4, 2)  # x1y1, x2y2, x1y2, x2y1\n",
    "        xy = xy @ matrix.T  # transform\n",
    "        xy = xy[:, :2].reshape(n, 8)  # perspective rescale or affine\n",
    "\n",
    "        # create new boxes\n",
    "        x = xy[:, [0, 2, 4, 6]]\n",
    "        y = xy[:, [1, 3, 5, 7]]\n",
    "        new = numpy.concatenate((x.min(1), y.min(1), x.max(1), y.max(1))).reshape(4, n).T\n",
    "\n",
    "        # clip\n",
    "        new[:, [0, 2]] = new[:, [0, 2]].clip(0, w)\n",
    "        new[:, [1, 3]] = new[:, [1, 3]].clip(0, h)\n",
    "\n",
    "        # filter candidates\n",
    "        indices = candidates(box1=targets[:, 1:5].T * s, box2=new.T)\n",
    "        targets = targets[indices]\n",
    "        targets[:, 1:5] = new[indices]\n",
    "\n",
    "    return samples, targets\n",
    "\n",
    "\n",
    "def mix_up(image1, label1, image2, label2):\n",
    "    # Applies MixUp augmentation https://arxiv.org/pdf/1710.09412.pdf\n",
    "    alpha = numpy.random.beta(32.0, 32.0)  # mix-up ratio, alpha=beta=32.0\n",
    "    image = (image1 * alpha + image2 * (1 - alpha)).astype(numpy.uint8)\n",
    "    label = numpy.concatenate((label1, label2), 0)\n",
    "    return image, label\n",
    "\n",
    "\n",
    "class Albumentations:\n",
    "    def __init__(self):\n",
    "        self.transform = None\n",
    "        try:\n",
    "            import albumentations as album\n",
    "\n",
    "            transforms = [album.Blur(p=0.01),\n",
    "                          album.CLAHE(p=0.01),\n",
    "                          album.ToGray(p=0.01),\n",
    "                          album.MedianBlur(p=0.01)]\n",
    "            self.transform = album.Compose(transforms,\n",
    "                                           album.BboxParams('yolo', ['class_labels']))\n",
    "\n",
    "        except ImportError:  # package not installed, skip\n",
    "            pass\n",
    "\n",
    "    def __call__(self, image, label):\n",
    "        if self.transform:\n",
    "            x = self.transform(image=image,\n",
    "                               bboxes=label[:, 1:],\n",
    "                               class_labels=label[:, 0])\n",
    "            image = x['image']\n",
    "            label = numpy.array([[c, *b] for c, b in zip(x['class_labels'], x['bboxes'])])\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import copy\n",
    "import csv\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "import numpy\n",
    "import torch\n",
    "import tqdm\n",
    "import yaml\n",
    "from torch.utils import data\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "def learning_rate(args, params):\n",
    "    def fn(x):\n",
    "        return (1 - x / args.epochs) * (1.0 - params['lrf']) + params['lrf']\n",
    "\n",
    "    return fn\n",
    "\n",
    "\n",
    "def train(args, params):\n",
    "    # Model\n",
    "    model = yolo_v8_n(len(params['names'].values())).cuda()\n",
    "\n",
    "    # Optimizer\n",
    "    accumulate = max(round(64 / (args.batch_size * args.world_size)), 1)\n",
    "    params['weight_decay'] *= args.batch_size * args.world_size * accumulate / 64\n",
    "\n",
    "    p = [], [], []\n",
    "    for v in model.modules():\n",
    "        if hasattr(v, 'bias') and isinstance(v.bias, torch.nn.Parameter):\n",
    "            p[2].append(v.bias)\n",
    "        if isinstance(v, torch.nn.BatchNorm2d):\n",
    "            p[1].append(v.weight)\n",
    "        elif hasattr(v, 'weight') and isinstance(v.weight, torch.nn.Parameter):\n",
    "            p[0].append(v.weight)\n",
    "\n",
    "    optimizer = torch.optim.SGD(p[2], params['lr0'], params['momentum'], nesterov=True)\n",
    "\n",
    "    optimizer.add_param_group({'params': p[0], 'weight_decay': params['weight_decay']})\n",
    "    optimizer.add_param_group({'params': p[1]})\n",
    "    del p\n",
    "\n",
    "    # Scheduler\n",
    "    lr = learning_rate(args, params)\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr, last_epoch=-1)\n",
    "\n",
    "    # EMA\n",
    "    ema = EMA(model) if args.local_rank == 0 else None\n",
    "\n",
    "    filenames = []\n",
    "    with open('./COCO/train2017.txt') as reader:\n",
    "        for filename in reader.readlines():\n",
    "            filename = filename.rstrip().split('/')[-1]\n",
    "            filenames.append('./COCO/images/train2017/' + filename)\n",
    "\n",
    "    dataset = Dataset(filenames, args.input_size, params, True)\n",
    "\n",
    "    if args.world_size <= 1:\n",
    "        sampler = None\n",
    "    else:\n",
    "        sampler = data.distributed.DistributedSampler(dataset)\n",
    "\n",
    "    loader = data.DataLoader(dataset, args.batch_size, sampler is None, sampler,\n",
    "                             num_workers=30, pin_memory=True, collate_fn=Dataset.collate_fn)\n",
    "\n",
    "    if args.world_size > 1:\n",
    "        # DDP mode\n",
    "        model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model)\n",
    "        model = torch.nn.parallel.DistributedDataParallel(module=model,\n",
    "                                                          device_ids=[args.local_rank],\n",
    "                                                          output_device=args.local_rank)\n",
    "\n",
    "    # Start training\n",
    "    best = 0\n",
    "    num_batch = len(loader)\n",
    "    amp_scale = torch.cuda.amp.GradScaler()\n",
    "    criterion = ComputeLoss(model, params)\n",
    "    num_warmup = max(round(params['warmup_epochs'] * num_batch), 1000)\n",
    "    with open('weights/step.csv', 'w') as f:\n",
    "        if args.local_rank == 0:\n",
    "            writer = csv.DictWriter(f, fieldnames=['epoch', 'mAP@50', 'mAP'])\n",
    "            writer.writeheader()\n",
    "        for epoch in range(args.epochs):\n",
    "            model.train()\n",
    "\n",
    "            if args.epochs - epoch == 10:\n",
    "                loader.dataset.mosaic = False\n",
    "\n",
    "            m_loss = AverageMeter()\n",
    "            if args.world_size > 1:\n",
    "                sampler.set_epoch(epoch)\n",
    "            p_bar = enumerate(loader)\n",
    "            if args.local_rank == 0:\n",
    "                print(('\\n' + '%10s' * 3) % ('epoch', 'memory', 'loss'))\n",
    "            if args.local_rank == 0:\n",
    "                p_bar = tqdm.tqdm(p_bar, total=num_batch)  # progress bar\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            for i, (samples, targets, _) in p_bar:\n",
    "                x = i + num_batch * epoch  # number of iterations\n",
    "                samples = samples.cuda().float() / 255\n",
    "                targets = targets.cuda()\n",
    "\n",
    "                # Warmup\n",
    "                if x <= num_warmup:\n",
    "                    xp = [0, num_warmup]\n",
    "                    fp = [1, 64 / (args.batch_size * args.world_size)]\n",
    "                    accumulate = max(1, numpy.interp(x, xp, fp).round())\n",
    "                    for j, y in enumerate(optimizer.param_groups):\n",
    "                        if j == 0:\n",
    "                            fp = [params['warmup_bias_lr'], y['initial_lr'] * lr(epoch)]\n",
    "                        else:\n",
    "                            fp = [0.0, y['initial_lr'] * lr(epoch)]\n",
    "                        y['lr'] = numpy.interp(x, xp, fp)\n",
    "                        if 'momentum' in y:\n",
    "                            fp = [params['warmup_momentum'], params['momentum']]\n",
    "                            y['momentum'] = numpy.interp(x, xp, fp)\n",
    "\n",
    "                # Forward\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    outputs = model(samples)  # forward\n",
    "                loss = criterion(outputs, targets)\n",
    "\n",
    "                m_loss.update(loss.item(), samples.size(0))\n",
    "\n",
    "                loss *= args.batch_size  # loss scaled by batch_size\n",
    "                loss *= args.world_size  # gradient averaged between devices in DDP mode\n",
    "\n",
    "                # Backward\n",
    "                amp_scale.scale(loss).backward()\n",
    "\n",
    "                # Optimize\n",
    "                if x % accumulate == 0:\n",
    "                    amp_scale.unscale_(optimizer)  # unscale gradients\n",
    "                    clip_gradients(model)  # clip gradients\n",
    "                    amp_scale.step(optimizer)  # optimizer.step\n",
    "                    amp_scale.update()\n",
    "                    optimizer.zero_grad()\n",
    "                    if ema:\n",
    "                        ema.update(model)\n",
    "\n",
    "                # Log\n",
    "                if args.local_rank == 0:\n",
    "                    memory = f'{torch.cuda.memory_reserved() / 1E9:.3g}G'  # (GB)\n",
    "                    s = ('%10s' * 2 + '%10.4g') % (f'{epoch + 1}/{args.epochs}', memory, m_loss.avg)\n",
    "                    p_bar.set_description(s)\n",
    "\n",
    "                del loss\n",
    "                del outputs\n",
    "\n",
    "            # Scheduler\n",
    "            scheduler.step()\n",
    "\n",
    "            if args.local_rank == 0:\n",
    "                # mAP\n",
    "                last = test(args, params, ema.ema)\n",
    "                writer.writerow({'mAP': str(f'{last[1]:.3f}'),\n",
    "                                 'epoch': str(epoch + 1).zfill(3),\n",
    "                                 'mAP@50': str(f'{last[0]:.3f}')})\n",
    "                f.flush()\n",
    "\n",
    "                # Update best mAP\n",
    "                if last[1] > best:\n",
    "                    best = last[1]\n",
    "\n",
    "                # Save model\n",
    "                ckpt = {'model': copy.deepcopy(ema.ema).half()}\n",
    "\n",
    "                # Save last, best and delete\n",
    "                torch.save(ckpt, './weights/last.pt')\n",
    "                if best == last[1]:\n",
    "                    torch.save(ckpt, './weights/best.pt')\n",
    "                del ckpt\n",
    "\n",
    "    if args.local_rank == 0:\n",
    "        strip_optimizer('./weights/best.pt')  # strip optimizers\n",
    "        strip_optimizer('./weights/last.pt')  # strip optimizers\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(args, params, model=None):\n",
    "    filenames = []\n",
    "    with open('./COCO/val2017.txt') as reader:\n",
    "        for filename in reader.readlines():\n",
    "            filename = filename.rstrip().split('/')[-1]\n",
    "            filenames.append('./COCO/images/val2017/' + filename)\n",
    "\n",
    "    dataset = Dataset(filenames, args.input_size, params, False)\n",
    "    loader = data.DataLoader(dataset, 8, False, num_workers=8,\n",
    "                             pin_memory=True, collate_fn=Dataset.collate_fn)\n",
    "\n",
    "    if model is None:\n",
    "        model = torch.load('./weights/best.pt', map_location='cuda')['model'].float()\n",
    "\n",
    "    model.half()\n",
    "    model.eval()\n",
    "\n",
    "    # Configure\n",
    "    iou_v = torch.linspace(0.5, 0.95, 10).cuda()  # iou vector for mAP@0.5:0.95\n",
    "    n_iou = iou_v.numel()\n",
    "\n",
    "    m_pre = 0.\n",
    "    m_rec = 0.\n",
    "    map50 = 0.\n",
    "    mean_ap = 0.\n",
    "    metrics = []\n",
    "    p_bar = tqdm.tqdm(loader, desc=('%10s' * 3) % ('precision', 'recall', 'mAP'))\n",
    "    for samples, targets, shapes in p_bar:\n",
    "        samples = samples.cuda()\n",
    "        targets = targets.cuda()\n",
    "        samples = samples.half()  # uint8 to fp16/32\n",
    "        samples = samples / 255  # 0 - 255 to 0.0 - 1.0\n",
    "        _, _, height, width = samples.shape  # batch size, channels, height, width\n",
    "\n",
    "        # Inference\n",
    "        outputs = model(samples)\n",
    "\n",
    "        # NMS\n",
    "        targets[:, 2:] *= torch.tensor((width, height, width, height)).cuda()  # to pixels\n",
    "        outputs = non_max_suppression(outputs, 0.001, 0.65)\n",
    "\n",
    "        # Metrics\n",
    "        for i, output in enumerate(outputs):\n",
    "            labels = targets[targets[:, 0] == i, 1:]\n",
    "            correct = torch.zeros(output.shape[0], n_iou, dtype=torch.bool).cuda()\n",
    "\n",
    "            if output.shape[0] == 0:\n",
    "                if labels.shape[0]:\n",
    "                    metrics.append((correct, *torch.zeros((3, 0)).cuda()))\n",
    "                continue\n",
    "\n",
    "            detections = output.clone()\n",
    "            scale(detections[:, :4], samples[i].shape[1:], shapes[i][0], shapes[i][1])\n",
    "\n",
    "            # Evaluate\n",
    "            if labels.shape[0]:\n",
    "                tbox = labels[:, 1:5].clone()  # target boxes\n",
    "                tbox[:, 0] = labels[:, 1] - labels[:, 3] / 2  # top left x\n",
    "                tbox[:, 1] = labels[:, 2] - labels[:, 4] / 2  # top left y\n",
    "                tbox[:, 2] = labels[:, 1] + labels[:, 3] / 2  # bottom right x\n",
    "                tbox[:, 3] = labels[:, 2] + labels[:, 4] / 2  # bottom right y\n",
    "                scale(tbox, samples[i].shape[1:], shapes[i][0], shapes[i][1])\n",
    "\n",
    "                correct = numpy.zeros((detections.shape[0], iou_v.shape[0]))\n",
    "                correct = correct.astype(bool)\n",
    "\n",
    "                t_tensor = torch.cat((labels[:, 0:1], tbox), 1)\n",
    "                iou = box_iou(t_tensor[:, 1:], detections[:, :4])\n",
    "                correct_class = t_tensor[:, 0:1] == detections[:, 5]\n",
    "                for j in range(len(iou_v)):\n",
    "                    x = torch.where((iou >= iou_v[j]) & correct_class)\n",
    "                    if x[0].shape[0]:\n",
    "                        matches = torch.cat((torch.stack(x, 1), iou[x[0], x[1]][:, None]), 1)\n",
    "                        matches = matches.cpu().numpy()\n",
    "                        if x[0].shape[0] > 1:\n",
    "                            matches = matches[matches[:, 2].argsort()[::-1]]\n",
    "                            matches = matches[numpy.unique(matches[:, 1], return_index=True)[1]]\n",
    "                            matches = matches[numpy.unique(matches[:, 0], return_index=True)[1]]\n",
    "                        correct[matches[:, 1].astype(int), j] = True\n",
    "                correct = torch.tensor(correct, dtype=torch.bool, device=iou_v.device)\n",
    "            metrics.append((correct, output[:, 4], output[:, 5], labels[:, 0]))\n",
    "\n",
    "    # Compute metrics\n",
    "    metrics = [torch.cat(x, 0).cpu().numpy() for x in zip(*metrics)]  # to numpy\n",
    "    if len(metrics) and metrics[0].any():\n",
    "        tp, fp, m_pre, m_rec, map50, mean_ap = compute_ap(*metrics)\n",
    "\n",
    "    # Print results\n",
    "    print('%10.3g' * 3 % (m_pre, m_rec, mean_ap))\n",
    "\n",
    "    # Return results\n",
    "    model.float()  # for training\n",
    "    return map50, mean_ap\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     1/500     8.72G     11.95: 100%|██████████| 38/38 [00:14<00:00,  2.54it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:02<00:00, 14.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0.00136     0.105  0.000305\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     2/500     8.72G     11.48: 100%|██████████| 38/38 [00:12<00:00,  2.99it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:02<00:00, 14.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0.00162     0.118  0.000383\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     3/500     11.4G      11.2: 100%|██████████| 38/38 [00:13<00:00,  2.87it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:02<00:00, 13.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.000921    0.0611  0.000127\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     4/500     11.4G     10.96: 100%|██████████| 38/38 [00:11<00:00,  3.22it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:02<00:00, 14.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.000783    0.0511  0.000193\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     5/500     11.4G     10.85: 100%|██████████| 38/38 [00:12<00:00,  3.00it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:01<00:00, 17.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.000478    0.0492  0.000109\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     6/500     11.4G     10.67: 100%|██████████| 38/38 [00:12<00:00,  3.08it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:02<00:00, 12.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0.00188     0.106  0.000621\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     7/500     11.4G     10.41: 100%|██████████| 38/38 [00:12<00:00,  3.08it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:02<00:00, 11.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.504   0.00721   0.00047\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     8/500     11.4G      10.1: 100%|██████████| 38/38 [00:12<00:00,  2.95it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:02<00:00, 15.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.525    0.0336   0.00224\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     9/500     11.4G      9.76: 100%|██████████| 38/38 [00:12<00:00,  3.11it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:02<00:00, 12.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0.3    0.0333   0.00265\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    10/500     11.4G     9.339: 100%|██████████| 38/38 [00:11<00:00,  3.25it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:02<00:00, 11.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.416    0.0348   0.00448\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    11/500     11.4G     8.873: 100%|██████████| 38/38 [00:12<00:00,  3.09it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:02<00:00, 15.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.665    0.0293   0.00487\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    12/500     11.4G     8.397: 100%|██████████| 38/38 [00:11<00:00,  3.25it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:02<00:00, 15.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.433    0.0344   0.00818\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    13/500     11.4G     8.099: 100%|██████████| 38/38 [00:12<00:00,  3.00it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:02<00:00, 13.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.195    0.0797     0.016\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    14/500     11.4G     7.664: 100%|██████████| 38/38 [00:12<00:00,  3.10it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:02<00:00, 13.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0.0412    0.0713   0.00953\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    15/500     11.4G     7.415: 100%|██████████| 38/38 [00:11<00:00,  3.18it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:02<00:00, 14.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.292     0.134    0.0327\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    16/500     11.4G     7.201: 100%|██████████| 38/38 [00:11<00:00,  3.23it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:02<00:00, 13.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.119     0.102    0.0304\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    17/500     11.4G     6.998: 100%|██████████| 38/38 [00:12<00:00,  3.16it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:02<00:00, 14.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.126     0.125    0.0359\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    18/500     11.4G      6.92: 100%|██████████| 38/38 [00:11<00:00,  3.27it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:02<00:00, 14.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.246     0.122    0.0318\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    19/500     11.4G     6.749: 100%|██████████| 38/38 [00:11<00:00,  3.19it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:02<00:00, 13.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.271    0.0783    0.0365\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    20/500     11.4G     6.563: 100%|██████████| 38/38 [00:11<00:00,  3.31it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:02<00:00, 15.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.276     0.169    0.0517\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    21/500     11.4G     6.422: 100%|██████████| 38/38 [00:12<00:00,  3.12it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:02<00:00, 15.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.336     0.187    0.0511\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    22/500     11.4G     6.267: 100%|██████████| 38/38 [00:12<00:00,  3.08it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:02<00:00, 13.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.318     0.169    0.0636\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    23/500     11.4G     6.172: 100%|██████████| 38/38 [00:12<00:00,  2.94it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:02<00:00, 14.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0.32     0.184    0.0634\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    24/500     11.4G      6.09: 100%|██████████| 38/38 [00:13<00:00,  2.84it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:02<00:00, 16.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.406     0.195    0.0854\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    25/500     11.4G     5.983: 100%|██████████| 38/38 [00:12<00:00,  3.01it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:02<00:00, 13.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.298     0.179    0.0614\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    26/500     11.4G     5.851: 100%|██████████| 38/38 [00:11<00:00,  3.25it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:02<00:00, 15.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0.46     0.233     0.103\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    27/500     11.4G     5.749: 100%|██████████| 38/38 [00:11<00:00,  3.26it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:02<00:00, 13.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.314     0.192    0.0698\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    28/500     11.4G     5.635: 100%|██████████| 38/38 [00:12<00:00,  3.05it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:02<00:00, 15.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.478     0.267     0.128\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    29/500     11.4G     5.579: 100%|██████████| 38/38 [00:11<00:00,  3.27it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:02<00:00, 12.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.502     0.259     0.124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    30/500     11.4G     5.481: 100%|██████████| 38/38 [00:12<00:00,  3.10it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:02<00:00, 13.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.375     0.298     0.152\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    31/500     11.4G     5.414: 100%|██████████| 38/38 [00:12<00:00,  3.13it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:02<00:00, 14.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.436     0.312     0.157\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    32/500     11.4G     5.334: 100%|██████████| 38/38 [00:11<00:00,  3.18it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:02<00:00, 13.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.346     0.324     0.147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    33/500     11.4G     5.329: 100%|██████████| 38/38 [00:11<00:00,  3.23it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:02<00:00, 15.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.491     0.305     0.163\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    34/500     11.4G     5.239: 100%|██████████| 38/38 [00:12<00:00,  3.14it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:02<00:00, 15.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.518      0.35     0.186\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    35/500     11.4G     5.158: 100%|██████████| 38/38 [00:11<00:00,  3.18it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:02<00:00, 13.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.591     0.347     0.194\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    36/500     11.4G     5.073: 100%|██████████| 38/38 [00:12<00:00,  3.07it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:02<00:00, 15.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.547     0.353     0.189\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    37/500     11.4G      5.09: 100%|██████████| 38/38 [00:12<00:00,  3.13it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:02<00:00, 12.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.621     0.366     0.214\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    38/500     11.4G     4.946: 100%|██████████| 38/38 [00:12<00:00,  3.12it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:01<00:00, 16.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.584     0.387      0.22\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    39/500     11.4G     4.889: 100%|██████████| 38/38 [00:12<00:00,  3.02it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:02<00:00, 15.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.414     0.369     0.202\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    40/500     11.4G     4.886: 100%|██████████| 38/38 [00:12<00:00,  3.17it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:02<00:00, 14.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.444     0.373     0.209\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    41/500     11.4G     4.785: 100%|██████████| 38/38 [00:11<00:00,  3.36it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:02<00:00, 13.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.614     0.327     0.201\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    42/500     11.4G     4.759: 100%|██████████| 38/38 [00:12<00:00,  3.11it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:02<00:00, 13.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.528     0.345     0.217\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    43/500     11.4G     4.718: 100%|██████████| 38/38 [00:11<00:00,  3.27it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:02<00:00, 13.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0.43     0.378     0.212\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    44/500     11.4G     4.719: 100%|██████████| 38/38 [00:12<00:00,  3.00it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:02<00:00, 13.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.393     0.368     0.193\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    45/500     11.4G     4.599: 100%|██████████| 38/38 [00:11<00:00,  3.24it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:02<00:00, 14.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.608     0.338     0.199\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    46/500     11.4G     4.621: 100%|██████████| 38/38 [00:11<00:00,  3.30it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:02<00:00, 15.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.632     0.376     0.232\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    47/500     11.4G     4.572: 100%|██████████| 38/38 [00:11<00:00,  3.18it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:02<00:00, 15.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.428     0.406     0.243\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    48/500     11.4G      4.52: 100%|██████████| 38/38 [00:12<00:00,  3.13it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:02<00:00, 13.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.552      0.38     0.227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    49/500     11.4G     4.529: 100%|██████████| 38/38 [00:12<00:00,  3.10it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:02<00:00, 15.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.506     0.358     0.226\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    50/500     11.4G     4.466: 100%|██████████| 38/38 [00:11<00:00,  3.22it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:01<00:00, 17.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.514     0.432     0.248\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    51/500     11.4G     4.428: 100%|██████████| 38/38 [00:11<00:00,  3.25it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:02<00:00, 16.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.479     0.378     0.218\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    52/500     11.4G     4.422: 100%|██████████| 38/38 [00:12<00:00,  3.10it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:02<00:00, 12.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.547     0.418     0.256\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    53/500     11.4G      4.34: 100%|██████████| 38/38 [00:11<00:00,  3.20it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:02<00:00, 14.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.558     0.394     0.257\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    54/500     11.4G     4.331: 100%|██████████| 38/38 [00:12<00:00,  3.14it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:02<00:00, 15.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0.72     0.359     0.248\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    55/500     11.4G     4.349: 100%|██████████| 38/38 [00:11<00:00,  3.22it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:02<00:00, 14.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.578      0.43     0.274\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    56/500     11.4G     4.277: 100%|██████████| 38/38 [00:11<00:00,  3.19it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:02<00:00, 13.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.447     0.405     0.211\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    57/500     11.4G     4.299: 100%|██████████| 38/38 [00:11<00:00,  3.22it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:02<00:00, 13.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.753     0.395     0.286\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    58/500     11.4G     4.274: 100%|██████████| 38/38 [00:12<00:00,  3.16it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:02<00:00, 14.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.644     0.424     0.295\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    59/500     11.4G     4.215: 100%|██████████| 38/38 [00:12<00:00,  3.03it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:02<00:00, 15.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.524      0.47     0.275\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    60/500     11.4G     4.214: 100%|██████████| 38/38 [00:11<00:00,  3.23it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:02<00:00, 13.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.524     0.469     0.294\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    61/500     11.4G     4.152: 100%|██████████| 38/38 [00:11<00:00,  3.25it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:02<00:00, 12.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.507     0.423     0.252\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    62/500     11.4G     4.098: 100%|██████████| 38/38 [00:11<00:00,  3.35it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:02<00:00, 15.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.575     0.422     0.275\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    63/500     11.4G     4.172: 100%|██████████| 38/38 [00:11<00:00,  3.30it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:02<00:00, 12.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.657     0.433     0.291\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    64/500     11.4G     4.108: 100%|██████████| 38/38 [00:11<00:00,  3.25it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:02<00:00, 14.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.609     0.464     0.302\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    65/500     11.4G     4.095: 100%|██████████| 38/38 [00:11<00:00,  3.24it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:02<00:00, 14.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.693     0.481     0.284\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    66/500     11.4G      4.08: 100%|██████████| 38/38 [00:12<00:00,  3.08it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:02<00:00, 15.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.543     0.477     0.289\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    67/500     11.4G     4.049: 100%|██████████| 38/38 [00:11<00:00,  3.20it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:02<00:00, 13.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.743     0.453     0.305\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    68/500     11.4G     4.052: 100%|██████████| 38/38 [00:11<00:00,  3.38it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:02<00:00, 15.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.549     0.432     0.285\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    69/500     11.4G     4.073: 100%|██████████| 38/38 [00:11<00:00,  3.29it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:02<00:00, 12.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.551     0.425     0.291\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    70/500     11.4G     4.035: 100%|██████████| 38/38 [00:12<00:00,  3.16it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:02<00:00, 14.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.667     0.402     0.288\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    71/500     11.4G     4.033: 100%|██████████| 38/38 [00:11<00:00,  3.32it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:02<00:00, 12.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.581     0.467     0.285\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    72/500     11.4G     3.953: 100%|██████████| 38/38 [00:12<00:00,  3.12it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:02<00:00, 12.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.735     0.429     0.309\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    73/500     11.4G     4.011: 100%|██████████| 38/38 [00:11<00:00,  3.19it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:02<00:00, 14.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.535     0.472     0.293\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    74/500     11.4G     3.911: 100%|██████████| 38/38 [00:12<00:00,  3.03it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:02<00:00, 14.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.572      0.43      0.28\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    75/500     11.4G      3.87: 100%|██████████| 38/38 [00:11<00:00,  3.24it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:02<00:00, 14.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.696     0.389     0.285\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    76/500     11.4G     3.887: 100%|██████████| 38/38 [00:11<00:00,  3.19it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:02<00:00, 14.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.684     0.384     0.272\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    77/500     11.4G     3.879: 100%|██████████| 38/38 [00:11<00:00,  3.30it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:02<00:00, 12.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.668     0.445     0.313\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    78/500     11.4G      3.82: 100%|██████████| 38/38 [00:11<00:00,  3.26it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:02<00:00, 15.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.585     0.463     0.303\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    79/500     11.4G     3.877: 100%|██████████| 38/38 [00:11<00:00,  3.34it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:02<00:00, 15.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.556     0.473     0.307\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    80/500     11.4G     3.861: 100%|██████████| 38/38 [00:11<00:00,  3.34it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:02<00:00, 12.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.574     0.502     0.305\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    81/500     11.4G     3.837: 100%|██████████| 38/38 [00:12<00:00,  3.12it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:02<00:00, 14.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.688      0.46     0.318\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    82/500     11.4G     3.788: 100%|██████████| 38/38 [00:11<00:00,  3.32it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:02<00:00, 12.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.657     0.477     0.329\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    83/500     11.4G     3.794: 100%|██████████| 38/38 [00:12<00:00,  3.17it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:02<00:00, 15.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.586      0.48     0.313\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    84/500     11.4G     3.727: 100%|██████████| 38/38 [00:12<00:00,  3.12it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:02<00:00, 13.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.486     0.497     0.314\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    85/500     11.4G     3.762: 100%|██████████| 38/38 [00:11<00:00,  3.45it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:02<00:00, 13.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.615     0.444     0.303\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    86/500     11.4G     3.761: 100%|██████████| 38/38 [00:10<00:00,  3.55it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:01<00:00, 18.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.581     0.528     0.315\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    87/500     11.4G     3.719: 100%|██████████| 38/38 [00:10<00:00,  3.60it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:01<00:00, 17.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.725     0.475     0.345\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    88/500     11.4G     3.696: 100%|██████████| 38/38 [00:10<00:00,  3.48it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:02<00:00, 14.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.622     0.446     0.319\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    89/500     11.4G     3.659: 100%|██████████| 38/38 [00:11<00:00,  3.35it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:01<00:00, 17.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.705     0.443       0.3\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    90/500     11.4G     3.731: 100%|██████████| 38/38 [00:11<00:00,  3.44it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:02<00:00, 14.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.652     0.488     0.339\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    91/500     11.4G     3.715: 100%|██████████| 38/38 [00:11<00:00,  3.41it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:01<00:00, 16.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.674     0.493     0.331\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    92/500     11.4G     3.728: 100%|██████████| 38/38 [00:11<00:00,  3.33it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:01<00:00, 17.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.644     0.499      0.35\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    93/500     11.4G     3.687: 100%|██████████| 38/38 [00:11<00:00,  3.17it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:01<00:00, 17.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.794     0.429     0.331\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    94/500     11.4G     3.663: 100%|██████████| 38/38 [00:10<00:00,  3.52it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:01<00:00, 17.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0.72     0.482     0.345\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    95/500     11.4G     3.654: 100%|██████████| 38/38 [00:11<00:00,  3.36it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:02<00:00, 14.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.707     0.461     0.324\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    96/500     11.4G     3.616: 100%|██████████| 38/38 [00:12<00:00,  3.08it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:02<00:00, 15.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.613     0.485      0.31\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    97/500     11.4G     3.642: 100%|██████████| 38/38 [00:10<00:00,  3.48it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:02<00:00, 15.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.757      0.46     0.331\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    98/500     11.4G     3.617: 100%|██████████| 38/38 [00:10<00:00,  3.49it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:02<00:00, 16.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0.68     0.448     0.327\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    99/500     11.4G     3.609: 100%|██████████| 38/38 [00:10<00:00,  3.55it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:02<00:00, 16.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.748     0.458     0.339\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   100/500     11.4G     3.591: 100%|██████████| 38/38 [00:10<00:00,  3.47it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:02<00:00, 15.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.722     0.483     0.358\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   101/500     11.4G     3.561: 100%|██████████| 38/38 [00:11<00:00,  3.29it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:01<00:00, 16.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.687     0.489     0.348\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   102/500     11.4G     3.542: 100%|██████████| 38/38 [00:10<00:00,  3.49it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:02<00:00, 16.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.752      0.46     0.345\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   103/500     11.4G     3.553: 100%|██████████| 38/38 [00:11<00:00,  3.34it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:02<00:00, 13.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0.77     0.462     0.343\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   104/500     11.4G     3.517: 100%|██████████| 38/38 [00:11<00:00,  3.33it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:01<00:00, 16.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.745     0.413     0.321\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   105/500     11.4G     3.528: 100%|██████████| 38/38 [00:11<00:00,  3.39it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:01<00:00, 16.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.703     0.486     0.343\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   106/500     11.4G     3.521: 100%|██████████| 38/38 [00:12<00:00,  3.15it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:02<00:00, 16.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.814     0.412     0.311\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   107/500     11.4G     3.537: 100%|██████████| 38/38 [00:11<00:00,  3.38it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:02<00:00, 15.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.685      0.49     0.351\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   108/500     11.4G     3.519: 100%|██████████| 38/38 [00:12<00:00,  3.14it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:01<00:00, 17.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.595      0.48     0.328\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   109/500     11.4G     3.455: 100%|██████████| 38/38 [00:11<00:00,  3.38it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:01<00:00, 19.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.697     0.477     0.344\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   110/500     11.4G     3.481: 100%|██████████| 38/38 [00:11<00:00,  3.21it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:01<00:00, 17.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.775     0.449     0.324\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   111/500     11.4G     3.459: 100%|██████████| 38/38 [00:12<00:00,  3.11it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:02<00:00, 15.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.809      0.43     0.335\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   112/500     11.4G     3.445: 100%|██████████| 38/38 [00:11<00:00,  3.29it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:02<00:00, 15.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.673     0.494      0.35\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   113/500     11.4G      3.43: 100%|██████████| 38/38 [00:10<00:00,  3.52it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:02<00:00, 15.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.637     0.435     0.295\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   114/500     11.4G       3.4: 100%|██████████| 38/38 [00:12<00:00,  3.05it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:02<00:00, 15.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.649     0.504      0.36\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   115/500     11.4G     3.396: 100%|██████████| 38/38 [00:11<00:00,  3.35it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:01<00:00, 17.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.764     0.452     0.338\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   116/500     11.4G     3.422: 100%|██████████| 38/38 [00:11<00:00,  3.44it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:01<00:00, 16.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.615     0.476     0.328\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   117/500     11.4G      3.42: 100%|██████████| 38/38 [00:11<00:00,  3.37it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:01<00:00, 16.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.722     0.483     0.353\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   118/500     11.4G      3.39: 100%|██████████| 38/38 [00:11<00:00,  3.33it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:02<00:00, 14.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.643     0.547     0.352\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   119/500     11.4G     3.433: 100%|██████████| 38/38 [00:11<00:00,  3.31it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:02<00:00, 14.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.681     0.468     0.355\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   120/500     11.4G     3.345: 100%|██████████| 38/38 [00:11<00:00,  3.24it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:02<00:00, 12.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.645     0.529     0.363\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   121/500     11.4G     3.373: 100%|██████████| 38/38 [00:11<00:00,  3.35it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:02<00:00, 14.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.589     0.536     0.347\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   122/500     11.4G     3.356: 100%|██████████| 38/38 [00:11<00:00,  3.38it/s]\n",
      " precision    recall       mAP: 100%|██████████| 33/33 [00:01<00:00, 17.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.628     0.485     0.343\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/38 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m     params \u001b[38;5;241m=\u001b[39m yaml\u001b[38;5;241m.\u001b[39msafe_load(f)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mtrain:\n\u001b[0;32m---> 26\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mtest:\n\u001b[1;32m     28\u001b[0m     test(args, params)\n",
      "Cell \u001b[0;32mIn[4], line 103\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(args, params)\u001b[0m\n\u001b[1;32m     99\u001b[0m     p_bar \u001b[38;5;241m=\u001b[39m tqdm\u001b[38;5;241m.\u001b[39mtqdm(p_bar, total\u001b[38;5;241m=\u001b[39mnum_batch)  \u001b[38;5;66;03m# progress bar\u001b[39;00m\n\u001b[1;32m    101\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (samples, targets, _) \u001b[38;5;129;01min\u001b[39;00m p_bar:\n\u001b[1;32m    104\u001b[0m     x \u001b[38;5;241m=\u001b[39m i \u001b[38;5;241m+\u001b[39m num_batch \u001b[38;5;241m*\u001b[39m epoch  \u001b[38;5;66;03m# number of iterations\u001b[39;00m\n\u001b[1;32m    105\u001b[0m     samples \u001b[38;5;241m=\u001b[39m samples\u001b[38;5;241m.\u001b[39mcuda()\u001b[38;5;241m.\u001b[39mfloat() \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/py3.9/lib/python3.9/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1327\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1324\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1327\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1328\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1329\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1330\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1283\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1281\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m   1282\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_thread\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[0;32m-> 1283\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1284\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1285\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1131\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1119\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1120\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1128\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1129\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1131\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1132\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1133\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1134\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1135\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/py3.9/lib/python3.9/queue.py:180\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m    179\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m--> 180\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get()\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_full\u001b[38;5;241m.\u001b[39mnotify()\n",
      "File \u001b[0;32m~/miniconda3/envs/py3.9/lib/python3.9/threading.py:316\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 316\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    318\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "args = argparse.ArgumentParser()\n",
    "\n",
    "args.input_size = 640\n",
    "args.batch_size = 32\n",
    "args.epochs = 500\n",
    "args.train = True\n",
    "args.test = False\n",
    "args.local_rank = int(os.getenv('LOCAL_RANK', 0))\n",
    "args.world_size = int(os.getenv('WORLD_SIZE', 1))\n",
    "\n",
    "if args.world_size > 1:\n",
    "    torch.cuda.set_device(device=args.local_rank)\n",
    "    torch.distributed.init_process_group(backend='nccl', init_method='env://')\n",
    "\n",
    "if args.local_rank == 0:\n",
    "    if not os.path.exists('weights'):\n",
    "        os.makedirs('weights')\n",
    "\n",
    "setup_seed()\n",
    "setup_multi_processes()\n",
    "\n",
    "with open('args.yaml', errors='ignore') as f:\n",
    "    params = yaml.safe_load(f)\n",
    "\n",
    "if args.train:\n",
    "    train(args, params)\n",
    "if args.test:\n",
    "    test(args, params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
