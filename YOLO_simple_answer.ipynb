{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object detection (YOLOv8 PyTorch)\n",
    "\n",
    "![yolov8](https://github.com/ultralytics/docs/releases/download/0/yolov8-comparison-plots.avif)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "import csv\n",
    "import warnings\n",
    "import numpy as np\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "from torch.nn.functional import cross_entropy, one_hot\n",
    "from torch.utils import data\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'lr0': 0.001,            # 초기 learning rate (Adam에서 주로 1e-3)\n",
    "    'weight_decay': 0.0005,  # Adam weight decay\n",
    "    'box': 7.5,              # box loss 가중치\n",
    "    'cls': 0.5,              # cls loss 가중치\n",
    "    'dfl': 1.5,              # dfl loss 가중치\n",
    "    'hsv_h': 0.015,          # HSV Hue augmentation\n",
    "    'hsv_s': 0.70,           # HSV Saturation augmentation\n",
    "    'hsv_v': 0.40,           # HSV Value augmentation\n",
    "    'degrees': 0.0,          # 이미지 회전 범위\n",
    "    'translate': 0.1,        # 이미지 평행이동 범위\n",
    "    'scale': 0.5,            # 이미지 확대/축소 범위\n",
    "    'shear': 0.0,            # 이미지 Shear 범위\n",
    "    'flip_ud': 0.0,          # 상하 반전 확률\n",
    "    'flip_lr': 0.5,          # 좌우 반전 확률\n",
    "    'mosaic': 1.0,           # mosaic augmentation 확률\n",
    "    'mix_up': 0.0,           # mix_up augmentation 확률\n",
    "    'names': {               # 클래스 이름들\n",
    "        0: 'chair',\n",
    "        1: 'diningtable',\n",
    "        2: 'car',\n",
    "        3: 'motorbike',\n",
    "        4: 'bycicle',\n",
    "        5: 'bus',\n",
    "        6: 'bottle',\n",
    "        7: 'tvmonitor'\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_seed(seed=0):\n",
    "    \"\"\"\n",
    "    Setup random seed to ensure reproducibility.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(coords, shape1, shape2, ratio_pad=None):\n",
    "    \"\"\"\n",
    "    예측된 박스(coords)를 원본 이미지 크기(shape2)에 맞추어 스케일 조정합니다.\n",
    "    shape1은 현재 이미지의 shape, shape2는 원본의 shape.\n",
    "    \"\"\"\n",
    "    if ratio_pad is None:\n",
    "        gain = min(shape1[0] / shape2[0], shape1[1] / shape2[1]) \n",
    "        pad = ((shape1[1] - shape2[1] * gain) / 2, (shape1[0] - shape2[0] * gain) / 2)\n",
    "    else:\n",
    "        gain = ratio_pad[0][0]\n",
    "        pad = ratio_pad[1]\n",
    "\n",
    "    coords[:, [0, 2]] -= pad[0]  \n",
    "    coords[:, [1, 3]] -= pad[1]  \n",
    "    coords[:, :4] /= gain\n",
    "\n",
    "    coords[:, 0].clamp_(0, shape2[1])  \n",
    "    coords[:, 1].clamp_(0, shape2[0])  \n",
    "    coords[:, 2].clamp_(0, shape2[1])  \n",
    "    coords[:, 3].clamp_(0, shape2[0])  \n",
    "    return coords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_anchors(x, strides, offset=0.5):\n",
    "    \"\"\"\n",
    "    출력 feature map을 기반으로 앵커 좌표를 만드는 함수\n",
    "    \"\"\"\n",
    "    assert x is not None\n",
    "    anchor_points, stride_tensor = [], []\n",
    "    for i, stride in enumerate(strides):\n",
    "        _, _, h, w = x[i].shape\n",
    "        sx = torch.arange(end=w, dtype=x[i].dtype, device=x[i].device) + offset\n",
    "        sy = torch.arange(end=h, dtype=x[i].dtype, device=x[i].device) + offset\n",
    "        sy, sx = torch.meshgrid(sy, sx)\n",
    "        anchor_points.append(torch.stack((sx, sy), -1).view(-1, 2))\n",
    "        stride_tensor.append(torch.full((h * w, 1), stride, dtype=x[i].dtype, device=x[i].device))\n",
    "    return torch.cat(anchor_points), torch.cat(stride_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wh2xy_(x):\n",
    "    \"\"\"\n",
    "    [cx, cy, w, h] -> [x1, y1, x2, y2]\n",
    "    \"\"\"\n",
    "    y = x.clone()\n",
    "    y[..., 0] = x[..., 0] - x[..., 2] / 2\n",
    "    y[..., 1] = x[..., 1] - x[..., 3] / 2\n",
    "    y[..., 2] = x[..., 0] + x[..., 2] / 2\n",
    "    y[..., 3] = x[..., 1] + x[..., 3] / 2\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IoU(Intersection over Union)\n",
    "\n",
    "![IoU](https://blog.kakaocdn.net/dn/I9MIb/btq9eMfNYbF/KeQxOsQydbNkZuRNhoMv9k/img.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_iou(box1, box2):\n",
    "    \"\"\"\n",
    "    두 박스 집합의 IoU를 계산합니다.\n",
    "    box1: (N,4), box2: (M,4)\n",
    "    \"\"\"\n",
    "    (a1, a2), (b1, b2) = box1[:, None].chunk(2, 2), box2.chunk(2, 1)\n",
    "    intersection = (torch.min(a2, b2) - torch.max(a1, b1)).clamp(0).prod(2)\n",
    "\n",
    "    box1 = box1.T\n",
    "    box2 = box2.T\n",
    "    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "    return intersection / (area1[:, None] + area2 - intersection)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "def visualize_boxes_and_iou(box1, box2, iou_fn):\n",
    "    \"\"\"\n",
    "    위에서 만든 IoU 함수를 활용하여 두 박스의 교집합과 IoU를 시각화합니다.\n",
    "    Args:\n",
    "        box1 (Tensor): 첫 번째 박스 (1, 4) 크기.\n",
    "        box2 (Tensor): 두 번째 박스 (1, 4) 크기.\n",
    "        iou_fn (callable): IoU 계산.\n",
    "    \"\"\"\n",
    "    # IoU 계산\n",
    "    iou_value = iou_fn(box1.unsqueeze(0), box2.unsqueeze(0))\n",
    "\n",
    "    # 박스 시각화\n",
    "    fig, ax = plt.subplots(1, figsize=(8, 8))\n",
    "\n",
    "    # 박스 1\n",
    "    rect1 = patches.Rectangle((box1[0], box1[1]), box1[2] - box1[0], box1[3] - box1[1],\n",
    "                               linewidth=2, edgecolor='r', facecolor='none', label='Box1')\n",
    "    ax.add_patch(rect1)\n",
    "\n",
    "    # 박스 2\n",
    "    rect2 = patches.Rectangle((box2[0], box2[1]), box2[2] - box2[0], box2[3] - box2[1],\n",
    "                               linewidth=2, edgecolor='b', facecolor='none', label='Box2')\n",
    "    ax.add_patch(rect2)\n",
    "\n",
    "    # IoU 값 표시\n",
    "    ax.text(0.5, 1.05, f\"IoU: {iou_value.item():.4f}\", transform=ax.transAxes, fontsize=14,\n",
    "            verticalalignment='center', horizontalalignment='center', bbox=dict(facecolor='white', alpha=0.7))\n",
    "\n",
    "    # 설정\n",
    "    ax.set_xlim(0, max(box1[2], box2[2]) + 10)\n",
    "    ax.set_ylim(0, max(box1[3], box2[3]) + 10)\n",
    "    ax.set_aspect('equal')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# 예제\n",
    "box1 = torch.tensor([10, 10, 50, 50]) \n",
    "box2 = torch.tensor([30, 30, 70, 70]) \n",
    "\n",
    "visualize_boxes_and_iou(box1, box2, box_iou)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NMS (Non-Maximum Suppression)\n",
    "\n",
    "![nms1](https://wikidocs.net/images/page/142645/NMS.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import time\n",
    "\n",
    "# Non-Max Suppression 함수\n",
    "def non_max_suppression(prediction, conf_threshold=0.25, iou_threshold=0.45):\n",
    "    \"\"\"\n",
    "    Non-Maximum Suppression (NMS)를 수행하여 중복된 박스를 제거합니다.\n",
    "    Args:\n",
    "        prediction (Tensor): 모델 예측값 (batch_size, num_predictions, 4+num_classes).\n",
    "        conf_threshold (float): Confidence score 임계값.\n",
    "        iou_threshold (float): IoU 임계값.\n",
    "    Returns:\n",
    "        List[Tensor]: 이미지별 NMS가 적용된 박스 리스트.\n",
    "    \"\"\"\n",
    "    nc = prediction.shape[1] - 4  # 클래스 수\n",
    "    xc = prediction[:, 4:4 + nc].amax(1) > conf_threshold  # Confidence 필터링\n",
    "\n",
    "    max_wh = 7680  # 최대 offset\n",
    "    max_det = 300  # 최대 detection 개수\n",
    "    max_nms = 30000  # NMS에서 허용되는 최대 박스 수\n",
    "\n",
    "    start = time.time()\n",
    "    outputs = [torch.zeros((0, 6), device=prediction.device)] * prediction.shape[0]\n",
    "    for index, x in enumerate(prediction):\n",
    "        # Confidence score를 기반으로 박스 필터링\n",
    "        x = x.transpose(0, -1)[xc[index]]\n",
    "        if not x.shape[0]:\n",
    "            continue\n",
    "\n",
    "        # 좌표 변환\n",
    "        box, cls = x.split((4, nc), 1)\n",
    "        box = wh2xy_(box)\n",
    "\n",
    "        # 클래스 기반 필터링\n",
    "        if nc > 1:\n",
    "            i, j = (cls > conf_threshold).nonzero(as_tuple=False).T\n",
    "            x = torch.cat((box[i], x[i, 4 + j, None], j[:, None].float()), 1)\n",
    "        else:\n",
    "            conf, j = cls.max(1, keepdim=True)\n",
    "            x = torch.cat((box, conf, j.float()), 1)[conf.view(-1) > conf_threshold]\n",
    "\n",
    "        if not x.shape[0]:\n",
    "            continue\n",
    "\n",
    "        # NMS를 위한 상위 박스 선택\n",
    "        x = x[x[:, 4].argsort(descending=True)[:max_nms]]\n",
    "        c = x[:, 5:6] * max_wh  # offset 추가\n",
    "        boxes, scores = x[:, :4] + c, x[:, 4]\n",
    "\n",
    "        # NMS 수행\n",
    "        i = torchvision.ops.nms(boxes, scores, iou_threshold)\n",
    "        i = i[:max_det]  # 최대 detection 수 제한\n",
    "        outputs[index] = x[i]\n",
    "\n",
    "        # 타임아웃 확인\n",
    "        if (time.time() - start) > 0.5 + 0.05 * prediction.shape[0]:\n",
    "            print(f'WARNING ⚠️ NMS time limit exceeded')\n",
    "            break\n",
    "\n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth(y, f=0.05):\n",
    "    \"\"\"\n",
    "    박스 필터 기반 스무딩\n",
    "    \"\"\"\n",
    "    nf = round(len(y) * f * 2) // 2 + 1\n",
    "    p = np.ones(nf // 2)\n",
    "    yp = np.concatenate((p * y[0], y, p * y[-1]), 0)\n",
    "    return np.convolve(yp, np.ones(nf) / nf, mode='valid')\n",
    "\n",
    "\n",
    "def compute_ap(tp, conf, pred_cls, target_cls, eps=1e-16):\n",
    "    \"\"\"\n",
    "    mAP를 계산하는 함수.\n",
    "    \"\"\"\n",
    "    i = np.argsort(-conf)\n",
    "    tp, conf, pred_cls = tp[i], conf[i], pred_cls[i]\n",
    "    unique_classes, nt = np.unique(target_cls, return_counts=True)\n",
    "    nc = unique_classes.shape[0]\n",
    "\n",
    "    p = np.zeros((nc, 1000))\n",
    "    r = np.zeros((nc, 1000))\n",
    "    ap = np.zeros((nc, tp.shape[1]))\n",
    "    px = np.linspace(0, 1, 1000)\n",
    "\n",
    "    for ci, c in enumerate(unique_classes):\n",
    "        i = pred_cls == c\n",
    "        nl = nt[ci]\n",
    "        no = i.sum()\n",
    "        if no == 0 or nl == 0:\n",
    "            continue\n",
    "\n",
    "        fpc = (1 - tp[i]).cumsum(0)\n",
    "        tpc = tp[i].cumsum(0)\n",
    "        recall = tpc / (nl + eps)\n",
    "        r[ci] = np.interp(-px, -conf[i], recall[:, 0], left=0)\n",
    "        precision = tpc / (tpc + fpc)\n",
    "        p[ci] = np.interp(-px, -conf[i], precision[:, 0], left=1)\n",
    "\n",
    "        for j in range(tp.shape[1]):\n",
    "            m_rec = np.concatenate(([0.0], recall[:, j], [1.0]))\n",
    "            m_pre = np.concatenate(([1.0], precision[:, j], [0.0]))\n",
    "            m_pre = np.flip(np.maximum.accumulate(np.flip(m_pre)))\n",
    "            x = np.linspace(0, 1, 101)\n",
    "            ap[ci, j] = np.trapz(np.interp(x, m_rec, m_pre), x)\n",
    "\n",
    "    f1 = 2 * p * r / (p + r + eps)\n",
    "    i = smooth(f1.mean(0), 0.1).argmax()\n",
    "    p, r, f1 = p[:, i], r[:, i], f1[:, i]\n",
    "    tp = (r * nt).round()\n",
    "    fp = (tp / (p + eps) - tp).round()\n",
    "    ap50, ap_mean = ap[:, 0], ap.mean(1)\n",
    "    map50, mean_ap = ap50.mean(), ap_mean.mean()\n",
    "    return tp, fp, p.mean(), r.mean(), map50, mean_ap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_optimizer(filename):\n",
    "    \"\"\"\n",
    "    최종 배포 시 필요 없는 optimizer 상태 등 제거\n",
    "    \"\"\"\n",
    "    x = torch.load(filename, map_location=torch.device('cpu'))\n",
    "    x['model'].half()\n",
    "    for p in x['model'].parameters():\n",
    "        p.requires_grad = False\n",
    "    torch.save(x, filename)\n",
    "\n",
    "\n",
    "class AverageMeter:\n",
    "    \"\"\"\n",
    "    평균값을 간단히 관리하는 클래스\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.num = 0\n",
    "        self.sum = 0\n",
    "        self.avg = 0\n",
    "\n",
    "    def update(self, v, n):\n",
    "        if not math.isnan(float(v)):\n",
    "            self.num += n\n",
    "            self.sum += v * n\n",
    "            self.avg = self.sum / self.num\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss\n",
    "* Classification Loss (cls loss)\n",
    "    * Binary Cross Entropy (BCE)\n",
    "    * Use a weight of 0.5\n",
    "\n",
    "* IoU Loss (box loss)\n",
    "    * Complete IoU (CIoU)\n",
    "    * Use a weight of 7.5\n",
    "\n",
    "* Distribution Focal Loss (dfl loss)\n",
    "    * Predict the probability distribution of bounding box coordinates.\n",
    "    * Use a weight of 1.5\n",
    "    * ![dfl](https://velog.velcdn.com/images/d4r6j/post/26ffd850-8ea0-40df-a4b4-51e0053be4df/image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComputeLoss:\n",
    "    def __init__(self, model, params):\n",
    "        super().__init__()\n",
    "        if hasattr(model, 'module'):\n",
    "            model = model.module\n",
    "\n",
    "        device = next(model.parameters()).device\n",
    "        m = model.head  \n",
    "        self.bce = torch.nn.BCEWithLogitsLoss(reduction='none')\n",
    "        self.stride = m.stride\n",
    "        self.nc = m.nc\n",
    "        self.no = m.no\n",
    "        self.device = device\n",
    "        self.params = params\n",
    "        self.top_k = 10\n",
    "        self.alpha = 0.5\n",
    "        self.beta = 6.0\n",
    "        self.eps = 1e-9\n",
    "        self.bs = 1\n",
    "        self.num_max_boxes = 0\n",
    "        self.dfl_ch = m.dfl.ch\n",
    "        self.project = torch.arange(self.dfl_ch, dtype=torch.float, device=device)\n",
    "\n",
    "    def __call__(self, outputs, targets):\n",
    "        x = outputs[1] if isinstance(outputs, tuple) else outputs\n",
    "        output = torch.cat([i.view(x[0].shape[0], self.no, -1) for i in x], 2)\n",
    "        pred_output, pred_scores = output.split((4 * self.dfl_ch, self.nc), 1)\n",
    "\n",
    "        pred_output = pred_output.permute(0, 2, 1).contiguous()\n",
    "        pred_scores = pred_scores.permute(0, 2, 1).contiguous()\n",
    "\n",
    "        size = torch.tensor(x[0].shape[2:], dtype=pred_scores.dtype, device=self.device)\n",
    "        size = size * self.stride[0]\n",
    "        anchor_points, stride_tensor = make_anchors(x, self.stride, 0.5)\n",
    "\n",
    "        # targets\n",
    "        if targets.shape[0] == 0:\n",
    "            gt = torch.zeros(pred_scores.shape[0], 0, 5, device=self.device)\n",
    "        else:\n",
    "            i = targets[:, 0]\n",
    "            _, counts = i.unique(return_counts=True)\n",
    "            gt = torch.zeros(pred_scores.shape[0], counts.max(), 5, device=self.device)\n",
    "            for j in range(pred_scores.shape[0]):\n",
    "                matches = i == j\n",
    "                n = matches.sum()\n",
    "                if n:\n",
    "                    gt[j, :n] = targets[matches, 1:]\n",
    "            gt[..., 1:5] = wh2xy_(gt[..., 1:5].mul_(size[[1, 0, 1, 0]]))\n",
    "\n",
    "        gt_labels, gt_bboxes = gt.split((1, 4), 2)\n",
    "        mask_gt = gt_bboxes.sum(2, keepdim=True).gt_(0)\n",
    "\n",
    "        b, a, c = pred_output.shape\n",
    "        pred_bboxes = pred_output.view(b, a, 4, c // 4).softmax(3)\n",
    "        pred_bboxes = pred_bboxes.matmul(self.project.type(pred_bboxes.dtype))\n",
    "        a, b = torch.split(pred_bboxes, 2, -1)\n",
    "        pred_bboxes = torch.cat((anchor_points - a, anchor_points + b), -1)\n",
    "\n",
    "        scores = pred_scores.detach().sigmoid()\n",
    "        bboxes = (pred_bboxes.detach() * stride_tensor).type(gt_bboxes.dtype)\n",
    "        target_bboxes, target_scores, fg_mask = self.assign(scores, bboxes,\n",
    "                                                            gt_labels, gt_bboxes, mask_gt,\n",
    "                                                            anchor_points * stride_tensor)\n",
    "\n",
    "        target_bboxes /= stride_tensor\n",
    "        target_scores_sum = target_scores.sum()\n",
    "\n",
    "        # cls loss\n",
    "        loss_cls = self.bce(pred_scores, target_scores.to(pred_scores.dtype))\n",
    "        loss_cls = loss_cls.sum() / (target_scores_sum + self.eps)\n",
    "\n",
    "        # iou loss, dfl loss\n",
    "        loss_box = torch.zeros(1, device=self.device)\n",
    "        loss_dfl = torch.zeros(1, device=self.device)\n",
    "        if fg_mask.sum():\n",
    "            weight = torch.masked_select(target_scores.sum(-1), fg_mask).unsqueeze(-1)\n",
    "            loss_box = self.iou(pred_bboxes[fg_mask], target_bboxes[fg_mask])\n",
    "            loss_box = ((1.0 - loss_box) * weight).sum() / (target_scores_sum + self.eps)\n",
    "            a, b = torch.split(target_bboxes, 2, -1)\n",
    "            target_lt_rb = torch.cat((anchor_points - a, b - anchor_points), -1)\n",
    "            target_lt_rb = target_lt_rb.clamp(0, self.dfl_ch - 1.01)\n",
    "            loss_dfl = self.df_loss(pred_output[fg_mask].view(-1, self.dfl_ch), target_lt_rb[fg_mask])\n",
    "            loss_dfl = (loss_dfl * weight).sum() / (target_scores_sum + self.eps)\n",
    "\n",
    "        loss_cls *= self.params['cls']\n",
    "        loss_box *= self.params['box']\n",
    "        loss_dfl *= self.params['dfl']\n",
    "\n",
    "        return loss_cls + loss_box + loss_dfl\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def assign(self, pred_scores, pred_bboxes, true_labels, true_bboxes, true_mask, anchors):\n",
    "        self.bs = pred_scores.size(0)\n",
    "        self.num_max_boxes = true_bboxes.size(1)\n",
    "        if self.num_max_boxes == 0:\n",
    "            device = true_bboxes.device\n",
    "            target_bboxes = torch.zeros_like(pred_bboxes).to(device)\n",
    "            target_scores = torch.zeros_like(pred_scores).to(device)\n",
    "            fg_mask = torch.zeros_like(pred_scores[..., 0]).to(device)\n",
    "            return target_bboxes, target_scores, fg_mask\n",
    "\n",
    "        i = torch.zeros([2, self.bs, self.num_max_boxes], dtype=torch.long)\n",
    "        i[0] = torch.arange(end=self.bs).view(-1, 1).repeat(1, self.num_max_boxes)\n",
    "        i[1] = true_labels.long().squeeze(-1)\n",
    "        overlaps = self.iou(true_bboxes.unsqueeze(2), pred_bboxes.unsqueeze(1)).squeeze(3).clamp(0)\n",
    "        align_metric = pred_scores[i[0], :, i[1]].pow(self.alpha) * overlaps.pow(self.beta)\n",
    "\n",
    "        bs, n_boxes, _ = true_bboxes.shape\n",
    "        lt, rb = true_bboxes.view(-1, 1, 4).chunk(2, 2)\n",
    "        bbox_deltas = torch.cat((anchors[None] - lt, rb - anchors[None]), dim=2)\n",
    "        mask_in_gts = bbox_deltas.view(bs, n_boxes, anchors.shape[0], -1).amin(3).gt_(1e-9)\n",
    "        metrics = align_metric * mask_in_gts\n",
    "        top_k_metrics, top_k_indices = torch.topk(metrics, self.top_k, dim=-1, largest=True)\n",
    "        is_in_top_k = one_hot(top_k_indices, anchors.shape[0]).sum(-2)\n",
    "        is_in_top_k = torch.where(is_in_top_k > 1, 0, is_in_top_k)\n",
    "        mask_top_k = is_in_top_k.to(metrics.dtype)\n",
    "        mask_pos = mask_top_k * mask_in_gts * true_mask\n",
    "        fg_mask = mask_pos.sum(-2)\n",
    "        if fg_mask.max() > 1:\n",
    "            mask_multi_gts = (fg_mask.unsqueeze(1) > 1).repeat([1, self.num_max_boxes, 1])\n",
    "            max_overlaps_idx = overlaps.argmax(1)\n",
    "            is_max_overlaps = one_hot(max_overlaps_idx, self.num_max_boxes)\n",
    "            is_max_overlaps = is_max_overlaps.permute(0, 2, 1).to(overlaps.dtype)\n",
    "            mask_pos = torch.where(mask_multi_gts, is_max_overlaps, mask_pos)\n",
    "            fg_mask = mask_pos.sum(-2)\n",
    "        target_gt_idx = mask_pos.argmax(-2)\n",
    "        batch_index = torch.arange(end=self.bs, dtype=torch.int64, device=true_labels.device)[..., None]\n",
    "        target_gt_idx = target_gt_idx + batch_index * self.num_max_boxes\n",
    "        target_labels = true_labels.long().flatten()[target_gt_idx]\n",
    "        target_bboxes = true_bboxes.view(-1, 4)[target_gt_idx]\n",
    "        target_labels.clamp(0)\n",
    "        target_scores = one_hot(target_labels, self.nc)\n",
    "        fg_scores_mask = fg_mask[:, :, None].repeat(1, 1, self.nc)\n",
    "        target_scores = torch.where(fg_scores_mask > 0, target_scores, 0)\n",
    "        align_metric *= mask_pos\n",
    "        pos_align_metrics = align_metric.amax(axis=-1, keepdim=True)\n",
    "        pos_overlaps = (overlaps * mask_pos).amax(axis=-1, keepdim=True)\n",
    "        norm_align_metric = (align_metric * pos_overlaps / (pos_align_metrics + self.eps)).amax(-2).unsqueeze(-1)\n",
    "        target_scores = target_scores * norm_align_metric\n",
    "        return target_bboxes, target_scores, fg_mask.bool()\n",
    "\n",
    "    @staticmethod\n",
    "    def df_loss(pred_dist, target):\n",
    "        tl = target.long()\n",
    "        tr = tl + 1\n",
    "        wl = tr - target\n",
    "        wr = 1 - wl\n",
    "        l_loss = cross_entropy(pred_dist, tl.view(-1), reduction=\"none\").view(tl.shape)\n",
    "        r_loss = cross_entropy(pred_dist, tr.view(-1), reduction=\"none\").view(tl.shape)\n",
    "        return (l_loss * wl + r_loss * wr).mean(-1, keepdim=True)\n",
    "\n",
    "    @staticmethod\n",
    "    def iou(box1, box2, eps=1e-7):\n",
    "        b1_x1, b1_y1, b1_x2, b1_y2 = box1.chunk(4, -1)\n",
    "        b2_x1, b2_y1, b2_x2, b2_y2 = box2.chunk(4, -1)\n",
    "        w1, h1 = b1_x2 - b1_x1, b1_y2 - b1_y1 + eps\n",
    "        w2, h2 = b2_x2 - b2_x1, b2_y2 - b2_y1 + eps\n",
    "        area1 = b1_x2.minimum(b2_x2) - b1_x1.maximum(b2_x1)\n",
    "        area2 = b1_y2.minimum(b2_y2) - b1_y1.maximum(b2_y1)\n",
    "        intersection = area1.clamp(0) * area2.clamp(0)\n",
    "        union = w1 * h1 + w2 * h2 - intersection + eps\n",
    "        iou = intersection / union\n",
    "        cw = b1_x2.maximum(b2_x2) - b1_x1.minimum(b2_x1)\n",
    "        ch = b1_y2.maximum(b2_y2) - b1_y1.minimum(b2_y1)\n",
    "        c2 = cw ** 2 + ch ** 2 + eps\n",
    "        rho2 = ((b2_x1 + b2_x2 - b1_x1 - b1_x2) ** 2 + (b2_y1 + b2_y2 - b1_y1 - b1_y2) ** 2) / 4\n",
    "        v = (4 / math.pi ** 2) * (torch.atan(w2 / h2) - torch.atan(w1 / h1)).pow(2)\n",
    "        with torch.no_grad():\n",
    "            alpha = v / (v - iou + (1 + eps))\n",
    "        return iou - (rho2 / c2 + v * alpha)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_to_image(tensor):\n",
    "    \"\"\"\n",
    "    (C,H,W) 텐서를 (H,W,C) uint8 이미지로 변환\n",
    "    \"\"\"\n",
    "    tensor = tensor.squeeze().cpu().numpy()\n",
    "    return (tensor * 255).astype(np.uint8)\n",
    "\n",
    "def pad(k, p=None, d=1):\n",
    "    \"\"\"\n",
    "    커널 크기(k)와 dilation(d)에 따른 패딩 계산.\n",
    "    \"\"\"\n",
    "    if d > 1:\n",
    "        k = d * (k - 1) + 1\n",
    "    if p is None:\n",
    "        p = k // 2\n",
    "    return p\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "* Backbone-FPN-Head 구조\n",
    "\n",
    "![yolov8 model](https://github.com/Gwangiksin/ml_cv/blob/master/img/yolov8.png?raw=true)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic blocks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuse_conv(conv, norm):\n",
    "    \"\"\"\n",
    "    Conv와 BatchNorm 파라미터를 합치는 함수.\n",
    "    추론에서만 사용 (모델 경량화)\n",
    "    \"\"\"\n",
    "    fused_conv = torch.nn.Conv2d(conv.in_channels,\n",
    "                                 conv.out_channels,\n",
    "                                 kernel_size=conv.kernel_size,\n",
    "                                 stride=conv.stride,\n",
    "                                 padding=conv.padding,\n",
    "                                 groups=conv.groups,\n",
    "                                 bias=True).requires_grad_(False).to(conv.weight.device)\n",
    "\n",
    "    w_conv = conv.weight.clone().view(conv.out_channels, -1)\n",
    "    w_norm = torch.diag(norm.weight.div(torch.sqrt(norm.running_var + norm.eps)))\n",
    "    fused_conv.weight.copy_(torch.mm(w_norm, w_conv).view(fused_conv.weight.size()))\n",
    "\n",
    "    b_conv = torch.zeros(conv.weight.size(0), device=conv.weight.device) if conv.bias is None else conv.bias\n",
    "    b_norm = norm.bias - norm.weight.mul(norm.running_mean).div(torch.sqrt(norm.running_var + norm.eps))\n",
    "    fused_conv.bias.copy_(torch.mm(w_norm, b_conv.reshape(-1, 1)).reshape(-1) + b_norm)\n",
    "    return fused_conv\n",
    "\n",
    "\n",
    "class Conv(torch.nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, k=1, s=1, p=None, d=1, g=1):\n",
    "        super().__init__()\n",
    "        self.conv = torch.nn.Conv2d(in_ch, out_ch, k, s, pad(k, p, d), d, g, bias=False)\n",
    "        self.norm = torch.nn.BatchNorm2d(out_ch, 0.001, 0.03)\n",
    "        self.relu = torch.nn.SiLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.relu(self.norm(self.conv(x)))\n",
    "\n",
    "    def fuse_forward(self, x):\n",
    "        return self.relu(self.conv(x))\n",
    "    \n",
    "class Residual(torch.nn.Module):\n",
    "    def __init__(self, ch, add=True):\n",
    "        super().__init__()\n",
    "        self.add_m = add\n",
    "        self.res_m = torch.nn.Sequential(\n",
    "            Conv(ch, ch, 3),\n",
    "            Conv(ch, ch, 3)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.res_m(x) + x if self.add_m else self.res_m(x)\n",
    "\n",
    "\n",
    "class CSP(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Cross Stage Partial 네트워크 구성 블록\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch, out_ch, n=1, add=True):\n",
    "        super().__init__()\n",
    "        self.conv1 = Conv(in_ch, out_ch // 2)\n",
    "        self.conv2 = Conv(in_ch, out_ch // 2)\n",
    "        self.conv3 = Conv((2 + n) * out_ch // 2, out_ch)\n",
    "        self.res_m = torch.nn.ModuleList(Residual(out_ch // 2, add) for _ in range(n))\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = [self.conv1(x), self.conv2(x)]\n",
    "        y.extend(m(y[-1]) for m in self.res_m)\n",
    "        return self.conv3(torch.cat(y, dim=1))\n",
    "\n",
    "\n",
    "class SPP(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Spatial Pyramid Pooling\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch, out_ch, k=5):\n",
    "        super().__init__()\n",
    "        self.conv1 = Conv(in_ch, in_ch // 2)\n",
    "        self.conv2 = Conv(in_ch * 2, out_ch)\n",
    "        self.res_m = torch.nn.MaxPool2d(k, 1, k // 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        y1 = self.res_m(x)\n",
    "        y2 = self.res_m(y1)\n",
    "        return self.conv2(torch.cat([x, y1, y2, self.res_m(y2)], 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backbone & Neck\n",
    "![fpn](https://wikidocs.net/images/page/162976/FPN_1.PNG)\n",
    "\n",
    "#### TODO\n",
    "* Change the backbone from DarkNet to ResNet18."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DarkNet(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    YOLOv8 Backbone\n",
    "    \"\"\"\n",
    "    def __init__(self, width, depth):\n",
    "        super().__init__()\n",
    "        p1 = [Conv(width[0], width[1], 3, 2)]\n",
    "        p2 = [Conv(width[1], width[2], 3, 2), CSP(width[2], width[2], depth[0])]\n",
    "        p3 = [Conv(width[2], width[3], 3, 2), CSP(width[3], width[3], depth[1])]\n",
    "        p4 = [Conv(width[3], width[4], 3, 2), CSP(width[4], width[4], depth[2])]\n",
    "        p5 = [Conv(width[4], width[5], 3, 2), CSP(width[5], width[5], depth[0]), SPP(width[5], width[5])]\n",
    "\n",
    "        self.p1 = torch.nn.Sequential(*p1)\n",
    "        self.p2 = torch.nn.Sequential(*p2)\n",
    "        self.p3 = torch.nn.Sequential(*p3)\n",
    "        self.p4 = torch.nn.Sequential(*p4)\n",
    "        self.p5 = torch.nn.Sequential(*p5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        p1 = self.p1(x)\n",
    "        p2 = self.p2(p1)\n",
    "        p3 = self.p3(p2)\n",
    "        p4 = self.p4(p3)\n",
    "        p5 = self.p5(p4)\n",
    "        return p3, p4, p5\n",
    "\n",
    "\n",
    "class DarkFPN(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Feature Pyramid Network\n",
    "    \"\"\"\n",
    "    def __init__(self, width, depth):\n",
    "        super().__init__()\n",
    "        self.up = torch.nn.Upsample(None, 2)\n",
    "        self.h1 = CSP(width[4] + width[5], width[4], depth[0], False)\n",
    "        self.h2 = CSP(width[3] + width[4], width[3], depth[0], False)\n",
    "        self.h3 = Conv(width[3], width[3], 3, 2)\n",
    "        self.h4 = CSP(width[3] + width[4], width[4], depth[0], False)\n",
    "        self.h5 = Conv(width[4], width[4], 3, 2)\n",
    "        self.h6 = CSP(width[4] + width[5], width[5], depth[0], False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        p3, p4, p5 = x\n",
    "        h1 = self.h1(torch.cat([self.up(p5), p4], 1))\n",
    "        h2 = self.h2(torch.cat([self.up(h1), p3], 1))\n",
    "        h4 = self.h4(torch.cat([self.h3(h2), h1], 1))\n",
    "        h6 = self.h6(torch.cat([self.h5(h4), p5], 1))\n",
    "        return h2, h4, h6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DFL(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Integral module for Distribution Focal Loss\n",
    "    \"\"\"\n",
    "    def __init__(self, ch=16):\n",
    "        super().__init__()\n",
    "        self.ch = ch\n",
    "        self.conv = torch.nn.Conv2d(ch, 1, 1, bias=False).requires_grad_(False)\n",
    "        x = torch.arange(ch, dtype=torch.float).view(1, ch, 1, 1)\n",
    "        self.conv.weight.data[:] = torch.nn.Parameter(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, a = x.shape\n",
    "        x = x.view(b, 4, self.ch, a).transpose(2, 1)\n",
    "        return self.conv(x.softmax(1)).view(b, 4, a)\n",
    "\n",
    "\n",
    "class Head(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    YOLO detection head\n",
    "    \"\"\"\n",
    "    anchors = torch.empty(0)\n",
    "    strides = torch.empty(0)\n",
    "\n",
    "    def __init__(self, nc=80, filters=()):\n",
    "        super().__init__()\n",
    "        self.ch = 16\n",
    "        self.nc = nc\n",
    "        self.nl = len(filters)\n",
    "        self.no = nc + self.ch * 4\n",
    "        self.stride = torch.zeros(self.nl)\n",
    "        c1 = max(filters[0], self.nc)\n",
    "        c2 = max((filters[0] // 4, self.ch * 4))\n",
    "\n",
    "        self.dfl = DFL(self.ch)\n",
    "        self.cls = torch.nn.ModuleList(\n",
    "            torch.nn.Sequential(\n",
    "                Conv(x, c1, 3),\n",
    "                Conv(c1, c1, 3),\n",
    "                torch.nn.Conv2d(c1, self.nc, 1)\n",
    "            ) for x in filters\n",
    "        )\n",
    "        self.box = torch.nn.ModuleList(\n",
    "            torch.nn.Sequential(\n",
    "                Conv(x, c2, 3),\n",
    "                Conv(c2, c2, 3),\n",
    "                torch.nn.Conv2d(c2, 4 * self.ch, 1)\n",
    "            ) for x in filters\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i in range(self.nl):\n",
    "            x[i] = torch.cat((self.box[i](x[i]), self.cls[i](x[i])), 1)\n",
    "        if self.training:\n",
    "            return x\n",
    "        self.anchors, self.strides = (x.transpose(0, 1) for x in make_anchors(x, self.stride, 0.5))\n",
    "        x = torch.cat([i.view(x[0].shape[0], self.no, -1) for i in x], 2)\n",
    "        box, cls = x.split((self.ch * 4, self.nc), 1)\n",
    "        a, b = torch.split(self.dfl(box), 2, 1)\n",
    "        a = self.anchors.unsqueeze(0) - a\n",
    "        b = self.anchors.unsqueeze(0) + b\n",
    "        box = torch.cat(((a + b) / 2, b - a), 1)\n",
    "        return torch.cat((box * self.strides, cls.sigmoid()), 1)\n",
    "\n",
    "    def initialize_biases(self):\n",
    "        m = self\n",
    "        for a, b, s in zip(m.box, m.cls, m.stride):\n",
    "            a[-1].bias.data[:] = 1.0\n",
    "            b[-1].bias.data[:m.nc] = math.log(5 / m.nc / (640 / s) ** 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOLO(torch.nn.Module):\n",
    "    def __init__(self, width, depth, num_classes):\n",
    "        super().__init__()\n",
    "        self.net = DarkNet(width, depth)\n",
    "        self.fpn = DarkFPN(width, depth)\n",
    "        img_dummy = torch.zeros(1, 3, 256, 256)\n",
    "        self.head = Head(num_classes, (width[3], width[4], width[5]))\n",
    "        self.head.stride = torch.tensor([256 / x.shape[-2] for x in self.forward(img_dummy)])\n",
    "        self.stride = self.head.stride\n",
    "        self.head.initialize_biases()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        x = self.fpn(x)\n",
    "        return self.head(list(x))\n",
    "\n",
    "    def fuse(self):\n",
    "        \"\"\"\n",
    "        For inference\n",
    "        \"\"\"\n",
    "        for m in self.modules():\n",
    "            if type(m) is Conv and hasattr(m, 'norm'):\n",
    "                m.conv = fuse_conv(m.conv, m.norm)\n",
    "                m.forward = m.fuse_forward\n",
    "                delattr(m, 'norm')\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolo_v8_n(num_classes=80):\n",
    "    depth = [1, 2, 2]\n",
    "    width = [3, 16, 32, 64, 128, 256]\n",
    "    return YOLO(width, depth, num_classes)\n",
    "\n",
    "def yolo_v8_s(num_classes=80):\n",
    "    depth = [1, 2, 2]\n",
    "    width = [3, 32, 64, 128, 256, 512]\n",
    "    return YOLO(width, depth, num_classes)\n",
    "\n",
    "def yolo_v8_m(num_classes=80):\n",
    "    depth = [2, 4, 4]\n",
    "    width = [3, 48, 96, 192, 384, 576]\n",
    "    return YOLO(width, depth, num_classes)\n",
    "\n",
    "def yolo_v8_l(num_classes=80):\n",
    "    depth = [3, 6, 6]\n",
    "    width = [3, 64, 128, 256, 512, 512]\n",
    "    return YOLO(width, depth, num_classes)\n",
    "\n",
    "def yolo_v8_x(num_classes=80):\n",
    "    depth = [3, 6, 6]\n",
    "    width = [3, 80, 160, 320, 640, 640]\n",
    "    return YOLO(width, depth, num_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset & Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random perspective augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def candidates(box1, box2):\n",
    "    \"\"\"\n",
    "    유효한 Bounding Box 후보군 생성\n",
    "    \"\"\"\n",
    "    w1, h1 = box1[2] - box1[0], box1[3] - box1[1]\n",
    "    w2, h2 = box2[2] - box2[0], box2[3] - box2[1]\n",
    "    aspect_ratio = np.maximum(w2 / (h2 + 1e-16), h2 / (w2 + 1e-16))\n",
    "    return (w2 > 2) & (h2 > 2) & ((w2 * h2) / (w1 * h1 + 1e-16) > 0.1) & (aspect_ratio < 100)\n",
    "\n",
    "def random_perspective(samples, targets, params, border=(0, 0)):\n",
    "    \"\"\"\n",
    "    Random perspective transformation\n",
    "    matrix = translate @ shear @ rotate @ perspective @ center\n",
    "\n",
    "    TODO: Add perspective transform for targets\n",
    "    \"\"\"\n",
    "    \n",
    "    h = samples.shape[0] + border[0] * 2\n",
    "    w = samples.shape[1] + border[1] * 2\n",
    "    \n",
    "    # center: 이미지의 변환이 중심을 기준으로 이루어지도록 하는 행렬\n",
    "    '''\n",
    "    [1, 0, -w/2]\n",
    "    [0, 1, -h/2]\n",
    "    [0, 0, 1]\n",
    "    '''\n",
    "    center = np.eye(3)\n",
    "    center[0, 2] = -samples.shape[1] / 2\n",
    "    center[1, 2] = -samples.shape[0] / 2\n",
    "    \n",
    "    # perspective: 원근 변환 행렬(현재는 항등 행렬)\n",
    "    perspective = np.eye(3)\n",
    "    \n",
    "    # rotate: 회전 변환 행렬\n",
    "    '''\n",
    "    [cos(a), -sin(a), 0]\n",
    "    [sin(a), cos(a), 0]\n",
    "    [0, 0, 1]\n",
    "    '''\n",
    "    rotate = np.eye(3)\n",
    "    a = random.uniform(-params['degrees'], params['degrees'])\n",
    "    s = random.uniform(1 - params['scale'], 1 + params['scale'])\n",
    "    rotate[:2] = cv2.getRotationMatrix2D(angle=a, center=(0, 0), scale=s)\n",
    "    \n",
    "    # shear: 기울기 변환 행렬\n",
    "    '''\n",
    "    [1, tan(x), 0]\n",
    "    [tan(y), 1, 0]\n",
    "    [0, 0, 1]\n",
    "    '''\n",
    "    shear = np.eye(3)\n",
    "    shear[0, 1] = math.tan(random.uniform(-params['shear'], params['shear']) * math.pi / 180)\n",
    "    shear[1, 0] = math.tan(random.uniform(-params['shear'], params['shear']) * math.pi / 180)\n",
    "    \n",
    "    # translate: 이동 변환 행렬\n",
    "    '''\n",
    "    [1, 0, x]\n",
    "    [0, 1, y]\n",
    "    [0, 0, 1]\n",
    "    '''\n",
    "    translate = np.eye(3)\n",
    "    translate[0, 2] = random.uniform(0.5 - params['translate'], 0.5 + params['translate']) * w\n",
    "    translate[1, 2] = random.uniform(0.5 - params['translate'], 0.5 + params['translate']) * h\n",
    "    \n",
    "    # matrix: 변환 행렬\n",
    "    '''\n",
    "    행렬 적용 순서\n",
    "    1. center\n",
    "    2. perspective\n",
    "    3. rotate\n",
    "    4. shear\n",
    "    5. translate\n",
    "    '''\n",
    "    matrix = translate @ shear @ rotate @ perspective @ center\n",
    "    \n",
    "    \n",
    "    # 이미지 변환\n",
    "    if (border[0] != 0) or (border[1] != 0) or (matrix != np.eye(3)).any():\n",
    "        samples = cv2.warpAffine(samples, matrix[:2], (w, h), borderValue=(0, 0, 0))\n",
    "    \n",
    "    # 타겟 변환\n",
    "    n = len(targets)\n",
    "    if n:\n",
    "        xy = np.ones((n * 4, 3))\n",
    "        xy[:, :2] = targets[:, [1,2,3,4,1,4,3,2]].reshape(n*4, 2) # (x1, y1, x2, y2) -> (x1, y1), (x2, y2), (x1, y2), (x2, y1)\n",
    "        \n",
    "        ### TODO: Apply perspective transform for targets\n",
    "        xy = xy @ matrix.T\n",
    "        xy = xy[:, :2].reshape(n, 8)\n",
    "        x = xy[:, [0,2,4,6]]\n",
    "        y = xy[:, [1,3,5,7]]\n",
    "        \n",
    "        # new: (x1, y1, x2, y2)\n",
    "        new = np.concatenate((x.min(1), y.min(1), x.max(1), y.max(1))).reshape(4, n).T\n",
    "        \n",
    "        ### TODO: Clip new to image size\n",
    "        new[:, [0,2]] = new[:, [0,2]].clip(0, w)\n",
    "        new[:, [1,3]] = new[:, [1,3]].clip(0, h)\n",
    "        \n",
    "        idx = candidates(box1=targets[:,1:5].T * s, box2=new.T)\n",
    "        targets = targets[idx]\n",
    "        targets[:, 1:5] = new[idx]\n",
    "    return samples, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize\n",
    "def visualize_random_perspective(image, targets, params):\n",
    "    \"\"\"\n",
    "    random_perspective 적용 전후의 이미지를 시각화합니다.\n",
    "    Args:\n",
    "        image (ndarray): 원본 이미지.\n",
    "        targets (ndarray): 타겟 박스.\n",
    "        params (dict): 변형 파라미터.\n",
    "    \"\"\"\n",
    "    transformed_image, transformed_targets = random_perspective(image.copy(), targets.copy(), params)\n",
    "\n",
    "    # 시각화\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    # 원본 이미지\n",
    "    axes[0].imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    for t in targets:\n",
    "        x1, y1, x2, y2 = t[1:]\n",
    "        rect = plt.Rectangle((x1, y1), x2 - x1, y2 - y1, edgecolor='red', facecolor='none')\n",
    "        axes[0].add_patch(rect)\n",
    "    axes[0].set_title(\"Original Image\")\n",
    "\n",
    "    # 변환된 이미지\n",
    "    axes[1].imshow(cv2.cvtColor(transformed_image, cv2.COLOR_BGR2RGB))\n",
    "    for t in transformed_targets:\n",
    "        x1, y1, x2, y2 = t[1:]\n",
    "        rect = plt.Rectangle((x1, y1), x2 - x1, y2 - y1, edgecolor='blue', facecolor='none')\n",
    "        axes[1].add_patch(rect)\n",
    "    axes[1].set_title(\"Transformed Image\")\n",
    "    plt.show()\n",
    "\n",
    "# 예제\n",
    "dummy_image = cv2.imread('LOD_coco/images/train2017/1001.png')\n",
    "dummy_image = cv2.cvtColor(dummy_image, cv2.COLOR_BGR2RGB)\n",
    "dummy_image = cv2.resize(dummy_image, (640, 640))\n",
    "dummy_targets = np.array([[0, 15, 126, 300, 629]])\n",
    "visualize_random_perspective(dummy_image, dummy_targets, params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, filenames, input_size, params, augment):\n",
    "        self.params = params\n",
    "        self.mosaic = augment\n",
    "        self.augment = augment\n",
    "        self.input_size = input_size\n",
    "\n",
    "        # 레이블 로드\n",
    "        cache = self.load_label(filenames)\n",
    "        labels, shapes = zip(*cache.values())\n",
    "        self.labels = list(labels)\n",
    "        self.shapes = np.array(shapes, dtype=np.float64)\n",
    "        self.filenames = list(cache.keys()) \n",
    "        self.n = len(shapes)  \n",
    "        self.indices = range(self.n)\n",
    "        self.albumentations = Albumentations()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        index = self.indices[index]\n",
    "        params = self.params\n",
    "        mosaic = self.mosaic and random.random() < params['mosaic']\n",
    "\n",
    "        if mosaic:\n",
    "            image, label = self.load_mosaic(index, params)\n",
    "            # mixup\n",
    "            if random.random() < params['mix_up']:\n",
    "                index2 = random.choice(self.indices)\n",
    "                mix_image2, mix_label2 = self.load_mosaic(index2, params)\n",
    "                image, label = mix_up(image, label, mix_image2, mix_label2)\n",
    "        else:\n",
    "            image, shape = self.load_image(index)\n",
    "            h, w = image.shape[:2]\n",
    "            image, ratio, pad = resize(image, self.input_size, self.augment)\n",
    "            shapes = shape, ((h / shape[0], w / shape[1]), pad)\n",
    "            label = self.labels[index].copy()\n",
    "            if label.size:\n",
    "                label[:, 1:] = wh2xy(label[:, 1:], ratio[0] * w, ratio[1] * h, pad[0], pad[1])\n",
    "            if self.augment:\n",
    "                image, label = random_perspective(image, label, params)\n",
    "\n",
    "        nl = len(label)\n",
    "        if nl:\n",
    "            label[:, 1:5] = xy2wh(label[:, 1:5], image.shape[1], image.shape[0])\n",
    "\n",
    "        if self.augment:\n",
    "            image, label = self.albumentations(image, label)\n",
    "            nl = len(label)\n",
    "            augment_hsv(image, params)\n",
    "            if random.random() < params['flip_ud']:\n",
    "                image = np.flipud(image)\n",
    "                if nl:\n",
    "                    label[:, 2] = 1 - label[:, 2]\n",
    "            if random.random() < params['flip_lr']:\n",
    "                image = np.fliplr(image)\n",
    "                if nl:\n",
    "                    label[:, 1] = 1 - label[:, 1]\n",
    "\n",
    "        target = torch.zeros((nl, 6))\n",
    "        if nl:\n",
    "            target[:, 1:] = torch.from_numpy(label)\n",
    "\n",
    "        sample = image.transpose((2, 0, 1))[::-1]\n",
    "        sample = np.ascontiguousarray(sample)\n",
    "        return torch.from_numpy(sample), target, None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        samples, targets, _ = zip(*batch)\n",
    "        for i, item in enumerate(targets):\n",
    "            item[:, 0] = i\n",
    "        return torch.stack(samples, 0), torch.cat(targets, 0), None\n",
    "\n",
    "    @staticmethod\n",
    "    def load_label(filenames):\n",
    "        path = f'{os.path.dirname(filenames[0])}.cache'\n",
    "        if os.path.exists(path):\n",
    "            return torch.load(path)\n",
    "        x = {}\n",
    "        for filename in filenames:\n",
    "            try:\n",
    "                with open(filename, 'rb') as f:\n",
    "                    image = Image.open(f)\n",
    "                    image.verify()\n",
    "                shape = image.size\n",
    "                assert (shape[0] > 9) & (shape[1] > 9)\n",
    "                assert image.format.lower() in ('bmp','dng','jpeg','jpg','mpo','png','tif','tiff','webp')\n",
    "\n",
    "                a = f'{os.sep}images{os.sep}'\n",
    "                b = f'{os.sep}labels{os.sep}'\n",
    "                label_path = b.join(filename.rsplit(a, 1)).rsplit('.', 1)[0] + '.txt'\n",
    "                if os.path.isfile(label_path):\n",
    "                    with open(label_path) as f:\n",
    "                        label = [x.split() for x in f.read().strip().splitlines() if len(x)]\n",
    "                        label = np.array(label, dtype=np.float32)\n",
    "                    nl = len(label)\n",
    "                    if nl:\n",
    "                        assert label.shape[1] == 5\n",
    "                        assert (label >= 0).all()\n",
    "                        assert (label[:, 1:] <= 1).all()\n",
    "                        _, idx = np.unique(label, axis=0, return_index=True)\n",
    "                        if len(idx) < nl:\n",
    "                            label = label[idx]\n",
    "                    else:\n",
    "                        label = np.zeros((0, 5), dtype=np.float32)\n",
    "                else:\n",
    "                    label = np.zeros((0, 5), dtype=np.float32)\n",
    "                if filename:\n",
    "                    x[filename] = [label, shape]\n",
    "            except FileNotFoundError:\n",
    "                pass\n",
    "        torch.save(x, path)\n",
    "        return x\n",
    "\n",
    "    def load_image(self, i):\n",
    "        image = cv2.imread(self.filenames[i])\n",
    "        h, w = image.shape[:2]\n",
    "        r = self.input_size / max(h, w)\n",
    "        if r != 1:\n",
    "            image = cv2.resize(image, (int(w*r), int(h*r)), interpolation=resample() if self.augment else cv2.INTER_LINEAR)\n",
    "        return image, (h, w)\n",
    "\n",
    "    def load_mosaic(self, index, params):\n",
    "        label4 = []\n",
    "        image4 = np.full((self.input_size * 2, self.input_size * 2, 3), 0, dtype=np.uint8)\n",
    "        border = [-self.input_size // 2, -self.input_size // 2]\n",
    "        xc = int(random.uniform(-border[0], 2 * self.input_size + border[1]))\n",
    "        yc = int(random.uniform(-border[0], 2 * self.input_size + border[1]))\n",
    "        indices = [index] + random.choices(self.indices, k=3)\n",
    "        random.shuffle(indices)\n",
    "        for i, idx in enumerate(indices):\n",
    "            img, _ = self.load_image(idx)\n",
    "            shape = img.shape\n",
    "            if i == 0:  # top-left\n",
    "                x1a = max(xc - shape[1], 0)\n",
    "                y1a = max(yc - shape[0], 0)\n",
    "                x2a = xc\n",
    "                y2a = yc\n",
    "                x1b = shape[1] - (x2a - x1a)\n",
    "                y1b = shape[0] - (y2a - y1a)\n",
    "                x2b = shape[1]\n",
    "                y2b = shape[0]\n",
    "            elif i == 1:  # top-right\n",
    "                x1a = xc\n",
    "                y1a = max(yc - shape[0], 0)\n",
    "                x2a = min(xc + shape[1], self.input_size * 2)\n",
    "                y2a = yc\n",
    "                x1b = 0\n",
    "                y1b = shape[0] - (y2a - y1a)\n",
    "                x2b = min(shape[1], x2a - x1a)\n",
    "                y2b = shape[0]\n",
    "            elif i == 2:  # bottom-left\n",
    "                x1a = max(xc - shape[1], 0)\n",
    "                y1a = yc\n",
    "                x2a = xc\n",
    "                y2a = min(self.input_size * 2, yc + shape[0])\n",
    "                x1b = shape[1] - (x2a - x1a)\n",
    "                y1b = 0\n",
    "                x2b = shape[1]\n",
    "                y2b = min(y2a - y1a, shape[0])\n",
    "            else:         # bottom-right\n",
    "                x1a = xc\n",
    "                y1a = yc\n",
    "                x2a = min(xc + shape[1], self.input_size * 2)\n",
    "                y2a = min(self.input_size * 2, yc + shape[0])\n",
    "                x1b = 0\n",
    "                y1b = 0\n",
    "                x2b = min(shape[1], x2a - x1a)\n",
    "                y2b = min(y2a - y1a, shape[0])\n",
    "\n",
    "            image4[y1a:y2a, x1a:x2a] = img[y1b:y2b, x1b:x2b]\n",
    "            pad_w = x1a - x1b\n",
    "            pad_h = y1a - y1b\n",
    "            lb = self.labels[idx].copy()\n",
    "            if len(lb):\n",
    "                lb[:, 1:] = wh2xy(lb[:, 1:], shape[1], shape[0], pad_w, pad_h)\n",
    "            label4.append(lb)\n",
    "\n",
    "        label4 = np.concatenate(label4, 0)\n",
    "        for x in label4[:, 1:]:\n",
    "            np.clip(x, 0, 2*self.input_size, out=x)\n",
    "        image4, label4 = random_perspective(image4, label4, params, border)\n",
    "        return image4, label4\n",
    "\n",
    "\n",
    "def wh2xy(x, w=640, h=640, pad_w=0, pad_h=0):\n",
    "    y = np.copy(x)\n",
    "    y[:, 0] = w * (x[:, 0] - x[:, 2] / 2) + pad_w\n",
    "    y[:, 1] = h * (x[:, 1] - x[:, 3] / 2) + pad_h\n",
    "    y[:, 2] = w * (x[:, 0] + x[:, 2] / 2) + pad_w\n",
    "    y[:, 3] = h * (x[:, 1] + x[:, 3] / 2) + pad_h\n",
    "    return y\n",
    "\n",
    "\n",
    "def xy2wh(x, w=640, h=640):\n",
    "    x[:, [0, 2]] = x[:, [0, 2]].clip(0, w - 1E-3)\n",
    "    x[:, [1, 3]] = x[:, [1, 3]].clip(0, h - 1E-3)\n",
    "    y = np.copy(x)\n",
    "    y[:, 0] = ((x[:, 0] + x[:, 2]) / 2) / w\n",
    "    y[:, 1] = ((x[:, 1] + x[:, 3]) / 2) / h\n",
    "    y[:, 2] = (x[:, 2] - x[:, 0]) / w\n",
    "    y[:, 3] = (x[:, 3] - x[:, 1]) / h\n",
    "    return y\n",
    "\n",
    "\n",
    "def resample():\n",
    "    choices = (cv2.INTER_AREA, cv2.INTER_CUBIC, cv2.INTER_LINEAR, cv2.INTER_NEAREST, cv2.INTER_LANCZOS4)\n",
    "    return random.choice(choices)\n",
    "\n",
    "\n",
    "def augment_hsv(image, params):\n",
    "    h = params['hsv_h']\n",
    "    s = params['hsv_s']\n",
    "    v = params['hsv_v']\n",
    "    r = np.random.uniform(-1, 1, 3) * [h, s, v] + 1\n",
    "    hh, ss, vv = cv2.split(cv2.cvtColor(image, cv2.COLOR_BGR2HSV))\n",
    "    x = np.arange(0, 256, dtype=r.dtype)\n",
    "    lut_h = ((x * r[0]) % 180).astype('uint8')\n",
    "    lut_s = np.clip(x * r[1], 0, 255).astype('uint8')\n",
    "    lut_v = np.clip(x * r[2], 0, 255).astype('uint8')\n",
    "    im_hsv = cv2.merge((cv2.LUT(hh, lut_h), cv2.LUT(ss, lut_s), cv2.LUT(vv, lut_v)))\n",
    "    cv2.cvtColor(im_hsv, cv2.COLOR_HSV2BGR, dst=image)\n",
    "\n",
    "\n",
    "def resize(image, input_size, augment):\n",
    "    shape = image.shape[:2]\n",
    "    r = min(input_size / shape[0], input_size / shape[1])\n",
    "    if not augment:\n",
    "        r = min(r, 1.0)\n",
    "    pad = (int(round(shape[1] * r)), int(round(shape[0] * r)))\n",
    "    w = (input_size - pad[0]) / 2\n",
    "    h = (input_size - pad[1]) / 2\n",
    "    if shape[::-1] != pad:\n",
    "        image = cv2.resize(image, pad, interpolation=resample() if augment else cv2.INTER_LINEAR)\n",
    "    top, bottom = int(round(h - 0.1)), int(round(h + 0.1))\n",
    "    left, right = int(round(w - 0.1)), int(round(w + 0.1))\n",
    "    image = cv2.copyMakeBorder(image, top, bottom, left, right, cv2.BORDER_CONSTANT)\n",
    "    return image, (r, r), (w, h)\n",
    "\n",
    "\n",
    "\n",
    "def mix_up(image1, label1, image2, label2):\n",
    "    alpha = np.random.beta(32.0, 32.0)\n",
    "    image = (image1 * alpha + image2 * (1 - alpha)).astype(np.uint8)\n",
    "    label = np.concatenate((label1, label2), 0)\n",
    "    return image, label\n",
    "\n",
    "\n",
    "class Albumentations:\n",
    "    def __init__(self):\n",
    "        self.transform = None\n",
    "        try:\n",
    "            import albumentations as album\n",
    "            transforms = [\n",
    "                album.Blur(p=0.01),\n",
    "                album.CLAHE(p=0.01),\n",
    "                album.ToGray(p=0.01),\n",
    "                album.MedianBlur(p=0.01)\n",
    "            ]\n",
    "            self.transform = album.Compose(\n",
    "                transforms,\n",
    "                album.BboxParams('yolo', ['class_labels'])\n",
    "            )\n",
    "        except ImportError:\n",
    "            pass\n",
    "\n",
    "    def __call__(self, image, label):\n",
    "        if self.transform:\n",
    "            x = self.transform(image=image, bboxes=label[:,1:], class_labels=label[:,0])\n",
    "            image = x['image']\n",
    "            label = np.array([[c, *b] for c, b in zip(x['class_labels'], x['bboxes'])])\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epochs=10, batch_size=8, pretrained=False):\n",
    "    \"\"\"\n",
    "    Adam optimizer 기반으로 단순화된 학습 루프\n",
    "    \"\"\"\n",
    "    # 1) 모델 준비\n",
    "    if pretrained:\n",
    "        ckpt = torch.load(pretrained, map_location='cuda')\n",
    "        model = ckpt['model'].float()\n",
    "    else:\n",
    "        model = yolo_v8_n(num_classes=len(params['names'])).cuda()\n",
    "\n",
    "\n",
    "    # 2) 데이터셋 준비 (예시는 ./LOD_coco/train.txt 가정)\n",
    "    filenames = []\n",
    "    if os.path.exists('./LOD_coco/train2017.txt'):\n",
    "        with open('./LOD_coco/train2017.txt') as f:\n",
    "            for line in f.readlines():\n",
    "                filenames.append(line.strip())\n",
    "    dataset = Dataset(filenames, 640, params, augment=True)\n",
    "    loader = data.DataLoader(dataset, batch_size, shuffle=True,\n",
    "                             num_workers=8, pin_memory=True,\n",
    "                             collate_fn=Dataset.collate_fn)\n",
    "\n",
    "    # 3) 옵티마이저 (Adam)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=params['lr0'],\n",
    "                                 weight_decay=params['weight_decay'])\n",
    "\n",
    "    # 4) loss 함수\n",
    "    criterion = ComputeLoss(model, params)\n",
    "\n",
    "    best_map = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        loss_meter = AverageMeter()\n",
    "        for images, targets, _ in loader:\n",
    "            images = images.cuda().float() / 255.\n",
    "            targets = targets.cuda()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_meter.update(loss.item(), images.size(0))\n",
    "\n",
    "        print(f\"[Epoch {epoch+1}/{num_epochs}] Loss: {loss_meter.avg:.4f}\")\n",
    "\n",
    "        # 에폭마다 간단히 test 진행\n",
    "        map50, map_ = test(model)\n",
    "        print(f\"  -> mAP@0.5: {map50:.3f}, mAP@0.5:0.95: {map_:.3f}\")\n",
    "        if map_ > best_map:\n",
    "            best_map = map_\n",
    "            ckpt = {'model': copy.deepcopy(model).half()}\n",
    "            if not os.path.exists('weights'):\n",
    "                os.makedirs('weights')\n",
    "            # torch.save(ckpt, './weights/best.pt')\n",
    "    print(\"Training done!\")\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model=None):\n",
    "    \"\"\"\n",
    "    간단한 평가 루프 (LOD_coco/val.txt 가정)\n",
    "    \"\"\"\n",
    "    if model is None:\n",
    "        ckpt = torch.load('./weights/best.pt', map_location='cuda')\n",
    "        model = ckpt['model'].float()\n",
    "\n",
    "    model.eval()\n",
    "    filenames = []\n",
    "    if os.path.exists('./LOD_coco/val2017.txt'):\n",
    "        with open('./LOD_coco/val2017.txt') as f:\n",
    "            for line in f.readlines():\n",
    "                filenames.append(line.strip())\n",
    "\n",
    "    dataset = Dataset(filenames, 640, params, augment=False)\n",
    "    loader = data.DataLoader(dataset, batch_size=8, shuffle=False,\n",
    "                             num_workers=8, pin_memory=True,\n",
    "                             collate_fn=Dataset.collate_fn)\n",
    "    iou_v = torch.linspace(0.5, 0.95, 10).cuda()\n",
    "    n_iou = iou_v.numel()\n",
    "\n",
    "    metrics = []\n",
    "    for images, targets, _ in loader:\n",
    "        images = images.cuda().float() / 255.\n",
    "        targets = targets.cuda()\n",
    "        b, _, h, w = images.shape\n",
    "        outputs = model(images)\n",
    "\n",
    "        # NMS\n",
    "        targets[:, 2:] *= torch.tensor((w, h, w, h), device=targets.device)\n",
    "        outputs = non_max_suppression(outputs, 0.001, 0.65)\n",
    "\n",
    "        for i, output in enumerate(outputs):\n",
    "            labels = targets[targets[:, 0] == i, 1:]\n",
    "            correct = torch.zeros(output.shape[0], n_iou, dtype=torch.bool).cuda()\n",
    "            if output.shape[0] == 0:\n",
    "                if labels.shape[0]:\n",
    "                    metrics.append((correct, *torch.zeros((3, 0)).cuda()))\n",
    "                continue\n",
    "            detections = output.clone()\n",
    "            scale(detections[:, :4], images[i].shape[1:], (h, w), None)\n",
    "            if labels.shape[0]:\n",
    "                tbox = labels[:, 1:5].clone()\n",
    "                tbox[:, 0] = labels[:, 1] - labels[:, 3] / 2\n",
    "                tbox[:, 1] = labels[:, 2] - labels[:, 4] / 2\n",
    "                tbox[:, 2] = labels[:, 1] + labels[:, 3] / 2\n",
    "                tbox[:, 3] = labels[:, 2] + labels[:, 4] / 2\n",
    "                scale(tbox, images[i].shape[1:], (h, w), None)\n",
    "                correct_ = np.zeros((detections.shape[0], iou_v.shape[0]), dtype=bool)\n",
    "                t_tensor = torch.cat((labels[:, 0:1], tbox), 1)\n",
    "                iou = box_iou(t_tensor[:, 1:], detections[:, :4])\n",
    "                correct_class = t_tensor[:, 0:1] == detections[:, 5]\n",
    "                for j in range(len(iou_v)):\n",
    "                    x = torch.where((iou >= iou_v[j]) & correct_class)\n",
    "                    if x[0].shape[0]:\n",
    "                        matches = torch.cat((torch.stack(x, 1), iou[x[0], x[1]][:, None]), 1).cpu().numpy()\n",
    "                        if x[0].shape[0] > 1:\n",
    "                            matches = matches[matches[:,2].argsort()[::-1]]\n",
    "                            matches = matches[np.unique(matches[:,1], return_index=True)[1]]\n",
    "                            matches = matches[np.unique(matches[:,0], return_index=True)[1]]\n",
    "                        correct_[matches[:,1].astype(int), j] = True\n",
    "                correct = torch.tensor(correct_, dtype=torch.bool, device=iou_v.device)\n",
    "            metrics.append((correct, output[:, 4], output[:, 5], labels[:, 0]))\n",
    "\n",
    "    metrics = [torch.cat(x, 0).cpu().numpy() for x in zip(*metrics)]\n",
    "    if len(metrics) and metrics[0].any():\n",
    "        tp, fp, p, r, map50, mean_ap = compute_ap(*metrics)\n",
    "        return map50, mean_ap\n",
    "    else:\n",
    "        return 0., 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # 1) 시드 고정\n",
    "    setup_seed(0)\n",
    "    \n",
    "    # 2) Pretrained 모델 경로\n",
    "    pretrained = './weights/best.pt'\n",
    "    \n",
    "    # 3) 학습\n",
    "    train(num_epochs=5, batch_size=16, pretrained=pretrained)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
